{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Phrase Identification Assignment\n",
    "https://bcourses.berkeley.edu/courses/1376582/assignments/6650535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find the code to my key phrase identification algorithm. Section A contains the test code / exerimentation with different methods and section B contains the main algorithm that I am using to detect the key phrases.\n",
    "\n",
    "In my exerimentation, I tried to various methods starting with the term frequency with unigrams, bigrams, and trigrams. I felt that unigrams and bigrams did a decent job on my collection but, trigrams was a bit lackluster. I first attempted to run this without doing much data cleaning and compared the output of the freqdist after removing punctuation and stopwords and saw that there was only limited improvement. Next, I attempted collocations with the chi-squared method and this also did a decent job with bigrams but not such a great job with trigrams. The method that I spent the most time with was chunking. I began with a simple idea that I wanted to capture any combination of determiner + noun. I iterated on this grammar by checking the output with some changes until I settled on the grammar that you find below which I think does a good job of picking out some key phrases. There is some noise in the output but, overall, this gave me the best results. Finally, I also attempted to see if I could find a more general theme of the text using hyponyms for the most frequently occuring unigrams. However, this proved very diffcult as it was very noisy.\n",
    "\n",
    "For my final algorithm, I am mainly using my chunking method. I read in the text, break it into sentences, tokenize the sentences (without cleaning, because it helped with accurate identification of proper nouns), run through the chunker, then the freqdist of the chunks, and then, finally, normalize by lower casing and printing out a max of 2000 characters. This is a fairly straight forward method that I think does a decent job of finding the key terms. I think one thing that I can improve on is removing some of the noise from the text but, I didn't want to hurt recall trying to improve precision so, I left it alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(wrds):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    return list(filter(lambda w: w.lower() not in stop_words, wrds))\n",
    "\n",
    "def remove_punctuation(wrds):\n",
    "    exclude = list(string.punctuation) + [\"--\",\"...\", \"`\"]\n",
    "    return list(filter(lambda w: w[0] not in exclude, wrds))\n",
    "\n",
    "def remove_digits(wrds):\n",
    "    exclude = list(string.digits)\n",
    "    return list(filter(lambda w: w[0] not in exclude, wrds))\n",
    "\n",
    "def to_tokens(txt):\n",
    "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "        <\n",
    "        | :\n",
    "        | ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "       | \\w+([-']\\w+)*        # words with optional internal hyphens\n",
    "       | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "       | \\.\\.\\.            # ellipsis\n",
    "       | [.,;\"'?():-_`]+  # these are separate tokens\n",
    "     '''\n",
    "    return nltk.regexp_tokenize(txt, pattern)\n",
    "\n",
    "def to_sents(txt):\n",
    "    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    return sent_tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried various methods that we covered in class below individually to see how well, I could perform just using each single method. I wanted to use this to guide how I approached my final algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tal_text = open(\"../tal_stories/tal_text_clean.txt\")\n",
    "txt = tal_text.read()\n",
    "tal_text.close()\n",
    "tokens = to_tokens(txt)\n",
    "tokens = remove_stopwords(tokens)\n",
    "tokens = remove_punctuation(tokens)\n",
    "filtered_tokens = list(filter(lambda x: len(x) > 5, tokens))\n",
    "# filtered_tokens = map(lambda x: x.lower(), filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Frequent Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Most Common Unigrams\n",
      "[('people', 300), ('patent', 227), ('really', 149), ('Ventures', 120), ('Intellectual', 120), ('actually', 118), (\"didn't\", 111), ('company', 111), ('patents', 109), ('something', 106), ('things', 105), ('called', 102), ('companies', 101), (\"that's\", 101), (\"you're\", 99), ('little', 90), (\"That's\", 83), ('number', 81), (\"they're\", 80), ('started', 77), ('Anthony', 68), ('trying', 68), ('around', 67), (\"there's\", 65), ('anything', 63), ('business', 63), ('different', 61), ('someone', 60), ('thought', 59), (\"doesn't\", 58), ('internet', 56), ('getting', 55), ('program', 55), ('saying', 54), (\"There's\", 52), ('police', 51), ('problem', 50), ('wanted', 50), ('working', 49), ('numbers', 49), ('making', 49), ('cartel', 48), (\"wasn't\", 47), ('software', 47), ('another', 46), (\"They're\", 45), ('system', 45), ('somebody', 44), (\"wouldn't\", 44), ('probably', 44)]\n",
      "\n",
      " Most Common Bigrams\n",
      "[(('Intellectual', 'Ventures'), 120), ((\"don't\", 'know'), 58), (('Chris', 'Crawford'), 42), (('Public', 'Radio'), 40), (('Chris', \"Crawford's\"), 35), ((\"I'm\", 'going'), 32), (('American', 'Life'), 31), (('Radio', 'International'), 31), (('Oasis', 'Research'), 28), (('patent', 'troll'), 26), ((\"I'm\", 'Ira'), 25), (('Ira', 'Glass'), 25), (('New', 'York'), 23), (('Peter', 'Detkin'), 22), ((\"Crawford's\", 'patent'), 22), (('first', 'time'), 21), (('cancer', 'cells'), 19), (('little', 'bit'), 19), ((\"don't\", 'think'), 19), ((\"we're\", 'going'), 18), (('Silicon', 'Valley'), 18), (('feel', 'like'), 18), (('every', 'single'), 18), ((\"didn't\", 'know'), 17), (('Nathan', 'Myhrvold'), 17), (('think', \"it's\"), 17), ((\"don't\", 'want'), 17), (('company', 'called'), 16), ((\"It's\", 'like'), 16), (('Mike', 'Daisey'), 16), (('would', 'say'), 15), (('every', 'day'), 15), ((\"I'm\", 'like'), 14), (('Alex', 'Blumberg'), 14), (('lot', 'people'), 14), (('Adrian', 'Schoolcraft'), 13), (('years', 'ago'), 13), (('Life', 'distributed'), 13), ((\"he's\", 'like'), 13), ((\"it's\", 'like'), 13), (('Julian', 'Vinegas'), 12), (('one', 'thing'), 12), ((\"don't\", 'even'), 12), ((\"you're\", 'going'), 12), (('distributed', 'Public'), 12), (('looks', 'like'), 11), (('two', 'years'), 11), (('make', 'sure'), 11), (('81st', 'precinct'), 11), (('patent', 'system'), 11)]\n",
      "\n",
      " Most Common Trigrams\n",
      "[(('Public', 'Radio', 'International'), 31), ((\"I'm\", 'Ira', 'Glass'), 25), (('Chris', \"Crawford's\", 'patent'), 22), (('American', 'Life', 'distributed'), 13), (('Life', 'distributed', 'Public'), 12), (('distributed', 'Public', 'Radio'), 12), (('Life', \"I'm\", 'Ira'), 9), (('Chicago', 'Public', 'Radio'), 9), (('American', 'Life', \"I'm\"), 9), (('WBEZ', 'Chicago', \"it's\"), 8), (('International', 'program', 'continues'), 8), (('Chicago', \"it's\", 'American'), 8), (('week', 'stories', 'American'), 8), ((\"it's\", 'American', 'Life'), 8), (('Radio', 'International', 'program'), 8), (('PRI', 'Public', 'Radio'), 8), (('next', 'week', 'stories'), 8), (('International', \"I'm\", 'Ira'), 7), (('Radio', 'International', \"I'm\"), 7), (('continues', \"It's\", 'American'), 7), (('New', 'York', 'City'), 7), ((\"It's\", 'American', 'Life'), 7), ((\"don't\", 'know', \"it's\"), 7), (('program', 'continues', \"It's\"), 7), (('Radio', 'Public', 'Radio'), 7), (('Public', 'Radio', 'Public'), 7), (('senior', 'producer', 'Julie'), 6), (('program', 'produced', 'today'), 6), (('Back', 'next', 'week'), 6), (('Ventures', 'Computing', 'Platforce'), 6), (('Computing', 'Platforce', 'Assets'), 6), (('Nancy', 'Updike', 'senior'), 6), (('Intellectual', 'Ventures', 'says'), 6), (('producer', 'Julie', 'Snyder'), 6), (('Intellectual', 'Ventures', 'Computing'), 6), (('called', 'Intellectual', 'Ventures'), 6), (('website', \"don't\", 'know'), 6), (('two', 'years', 'ago'), 6), ((\"don't\", 'even', 'know'), 6), (('minute', 'Chicago', 'Public'), 6), (('Glass', 'Back', 'next'), 6), (('Life', 'PRI', 'Public'), 6), (('Ira', 'Glass', 'Back'), 6), (('American', 'Life', 'PRI'), 6), (('stories', 'American', 'Life'), 6), (('story', 'Intellectual', 'Ventures'), 6), (('company', 'Intellectual', 'Ventures'), 5), (('back', 'Intellectual', 'Ventures'), 5), (('reporter', 'Graham', 'Rayman'), 5), (('sold', 'Intellectual', 'Ventures'), 5)]\n"
     ]
    }
   ],
   "source": [
    "def run_freq_terms(input_tokens, n):\n",
    "    fq = nltk.FreqDist(input_tokens)  # compute the frequency distribution\n",
    "    return fq.most_common(n)\n",
    "\n",
    "print(\"\\n Most Common Unigrams\")\n",
    "print(run_freq_terms(filtered_tokens, 50))\n",
    "\n",
    "# bigrams\n",
    "bgs = nltk.bigrams(tokens)\n",
    "print(\"\\n Most Common Bigrams\")\n",
    "print(run_freq_terms(bgs, 50))\n",
    "\n",
    "# trigrams\n",
    "bgs = nltk.trigrams(tokens)\n",
    "print(\"\\n Most Common Trigrams\")\n",
    "print(run_freq_terms(bgs, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_collocations(input_tokens, collocation_type):\n",
    "    \n",
    "    if collocation_type == 'bigram':\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        finder = BigramCollocationFinder.from_words(input_tokens, 5)\n",
    "        finder.apply_freq_filter(5)\n",
    "\n",
    "        print('Printing Top 100 Collocations')\n",
    "        for item in finder.nbest(bigram_measures.chi_sq, 100):\n",
    "            print(item)\n",
    "\n",
    "    elif collocation_type == 'trigram':\n",
    "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "        finder = TrigramCollocationFinder.from_words(input_tokens, 7)\n",
    "        finder.apply_freq_filter(5)\n",
    "\n",
    "        print('Printing Top 100 Collocations')\n",
    "        for item in finder.nbest(trigram_measures.chi_sq, 100):\n",
    "            print(item)\n",
    "    \n",
    "    else:\n",
    "        print('You must choose either bigram or trigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Information obtained from Syntax aka Partial Parsing (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('podcasts', 'NNS')]\n",
      "[('some', 'DT'), ('interviews', 'NNS')]\n",
      "[('colleagues', 'NNS')]\n",
      "[('Zoe', 'NNP'), ('Chace', 'NNP')]\n",
      "[(\"She's\", 'NNS')]\n",
      "[('the', 'DT'), ('reporters', 'NNS')]\n",
      "[('Planet', 'NNP'), ('Money', 'NNP')]\n",
      "[('Zoe', 'NNP')]\n",
      "[('guys', 'NNS')]\n",
      "[('Jim', 'NNP'), ('Logan', 'NNP')]\n",
      "[('Richard', 'NNP'), ('Baker', 'NNP')]\n",
      "[('Personal', 'NNP'), ('Audio', 'NNP')]\n",
      "[('podcasts', 'NNS')]\n",
      "[('Zoe', 'NNP')]\n",
      "[('the', 'DT'), ('legalities', 'NNS')]\n",
      "[('Jim', 'NNP'), ('Logan', 'NNP')]\n",
      "[('the', 'DT'), ('inventors', 'NNS')]\n",
      "[(\"Here's\", 'NNS')]\n",
      "[('Personal', 'NNP'), ('Audio', 'NNP')]\n",
      "[('the', 'DT'), ('iPod', 'NNP')]\n",
      "[(\"it's\", 'NNP')]\n",
      "[('homes', 'NNS')]\n",
      "[('an', 'DT'), ('MP3', 'NNP')]\n",
      "[('all', 'DT'), ('kinds', 'NNS')]\n",
      "[('Jim', 'NNP')]\n",
      "[('these', 'DT'), ('words', 'NNS')]\n",
      "[('ideas', 'NNS')]\n",
      "[('newfangled', 'JJ'), ('MP3', 'NNP')]\n",
      "[('articles', 'NNS')]\n",
      "[('tapes', 'NNS')]\n",
      "[('first', 'JJ'), ('podcasts', 'NNS')]\n",
      "[('tapes', 'NNS')]\n",
      "[('These', 'DT'), ('guys', 'NNS')]\n",
      "[('Welcome', 'NNP')]\n",
      "[('\"A', 'NNP')]\n",
      "[('the', 'DT'), ('Best', 'NNP')]\n",
      "[('Popular', 'NNP'), ('Science', 'NNP'), (',\"', 'NNP')]\n",
      "[('Magazines', 'NNP')]\n",
      "[('Tape', 'NNP')]\n",
      "[('answers', 'NNS')]\n",
      "[('some', 'DT'), ('questions', 'NNS')]\n",
      "[('happens', 'NNS')]\n",
      "[('works', 'NNS')]\n",
      "[('these', 'DT'), ('guys', 'NNS')]\n",
      "[('podcasts', 'NNS')]\n",
      "[('other', 'JJ'), ('people', 'NNS')]\n",
      "[(\"It's\", 'NNS')]\n",
      "[('Jim', 'NNP'), ('Logan', 'NNP')]\n",
      "[('Zoe', 'NNP')]\n",
      "[('dollars', 'NNS')]\n",
      "[('years', 'NNS')]\n",
      "[('people', 'NNS')]\n",
      "[('years', 'NNS')]\n",
      "[('cassette', 'JJ'), ('tapes', 'NNS')]\n",
      "[('the', 'DT'), ('iPhone', 'NNP')]\n",
      "[('Well', 'NNP')]\n",
      "[('These', 'DT'), ('guys', 'NNS')]\n",
      "[('nonprofit', 'JJ'), ('podcasters', 'NNS')]\n",
      "[('fees', 'NNS')]\n",
      "[('businesses', 'NNS')]\n",
      "[('Adam', 'NNP'), ('Carolla', 'NNP')]\n",
      "[('The', 'DT'), ('Stuff', 'NNP'), ('You', 'NNP'), ('Should', 'NNP'), ('Know', 'NNP')]\n",
      "[('Discovery', 'NNP')]\n",
      "[('Marc', 'NNP'), (\"Maron's\", 'NNP')]\n",
      "[('WTF', 'NNP')]\n",
      "[('Maron', 'NNP')]\n",
      "[(\"There's\", 'NNS')]\n",
      "[('Richard', 'NNP'), ('Baker', 'NNP')]\n",
      "[('Personal', 'NNP'), ('Audio', 'NNP')]\n",
      "[(\"it's\", 'NNS')]\n",
      "[('licensees', 'NNS')]\n",
      "[('people', 'NNS')]\n",
      "[('many', 'JJ'), ('times', 'NNS')]\n",
      "[('downloads', 'NNS')]\n",
      "[('Can', 'NNP')]\n",
      "[('the', 'DT'), ('podcasters', 'NNS')]\n",
      "[(\"It's\", 'NNS')]\n",
      "[('these', 'DT'), ('podcasters', 'NNS')]\n",
      "[('Settlements', 'NNS')]\n",
      "[('Personal', 'NNP'), ('Audio', 'NNP')]\n",
      "[('Apple', 'NNP'), ('Computer', 'NNP')]\n",
      "[('companies', 'NNS')]\n",
      "[('settlements', 'NNS')]\n",
      "[('OK', 'NNP')]\n",
      "[('lawsuits', 'NNS')]\n",
      "[('Entrepreneurs', 'NNS')]\n",
      "[('engineers', 'NNS')]\n",
      "[('these', 'DT'), ('suits', 'NNS')]\n",
      "[('young', 'JJ'), ('companies', 'NNS')]\n",
      "[('years', 'NNS')]\n",
      "[('some', 'DT'), ('people', 'NNS')]\n",
      "[('patents', 'NNS')]\n",
      "[('other', 'JJ'), ('companies', 'NNS')]\n",
      "[('fees', 'NNS')]\n",
      "[('Chris', 'NNP'), ('Crawford', 'NNP')]\n",
      "[('all', 'DT'), ('kinds', 'NNS')]\n",
      "[('things', 'NNS')]\n",
      "[('these', 'DT'), ('lawsuits', 'NNS')]\n",
      "[('basic', 'JJ'), ('questions', 'NNS')]\n",
      "[('Chris', 'NNP'), ('Crawford', 'NNP')]\n",
      "[('companies', 'NNS')]\n",
      "[('Well', 'NNP')]\n",
      "[('years', 'NNS')]\n",
      "[('the', 'DT'), ('questions', 'NNS')]\n",
      "[('these', 'DT'), ('kinds', 'NNS')]\n",
      "[('cases', 'NNS')]\n",
      "[('years', 'NNS')]\n",
      "[('many', 'JJ'), ('facts', 'NNS')]\n",
      "[('the', 'DT'), ('answers', 'NNS')]\n",
      "[('the', 'DT'), ('mysteries', 'NNS')]\n",
      "[('years', 'NNS')]\n",
      "[('WBEZ', 'NNP'), ('Chicago', 'NNP')]\n",
      "[('This', 'DT'), ('American', 'NNP'), ('Life', 'NNP')]\n",
      "[('Public', 'NNP'), ('Radio', 'NNP'), ('International', 'NNP')]\n",
      "[('Ira', 'NNP'), ('Glass', 'NNP')]\n",
      "[('Stick', 'NNP')]\n",
      "[(\"I'm\", 'NNP')]\n",
      "[('things', 'NNS')]\n",
      "[('This', 'DT'), ('American', 'JJ'), ('Life', 'NNP')]\n",
      "[('Planet', 'NNP'), ('Money', 'NNP'), ('co-host', 'NNP'), ('Alex', 'NNP'), ('Blumberg', 'NNP')]\n",
      "[('NPR', 'NNP'), ('Digital', 'NNP'), ('Culture', 'NNP'), ('Correspondent', 'NNP'), ('Laura', 'NNP'), ('Sydell', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {(<DT|PP\\$>?<JJ>?<NN.>)*}\n",
    "      {<NNP>+}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentences = to_sents(txt)\n",
    "sentences = remove_punctuation(sentences)\n",
    "interesting_chunks = []\n",
    "\n",
    "for sentence in sentences[1:100]:\n",
    "    tree = cp.parse(nltk.pos_tag(to_tokens(sentence)))\n",
    "    for t in tree.subtrees():\n",
    "        if t.label() == 'NP':  \n",
    "            print(t.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using Semantic Similarity / Finding Higher-Level Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Attempt to find most common hyponyms for the given set of terms\n",
    "def categories_from_hypernyms(termlist, num_cats=20):\n",
    "    \n",
    "    hypterms = []\n",
    "    for term in termlist:                  # for each term\n",
    "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
    "        for syn in s:                      # for each lemma synset\n",
    "            hypterms += syn.hyponyms()\n",
    "                \n",
    "    hypfd = nltk.FreqDist(hypterms)\n",
    "    print(hypfd.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Synset('homebound.n.01'), 215), (Synset('living.n.02'), 215), (Synset('poor_people.n.01'), 215), (Synset('disabled.n.01'), 215), (Synset('nationality.n.01'), 215), (Synset('ionian.n.02'), 215), (Synset('populace.n.01'), 215), (Synset('laity.n.01'), 215), (Synset('ancients.n.01'), 215), (Synset('dorian.n.02'), 215), (Synset('migration.n.02'), 215), (Synset('damned.n.01'), 215), (Synset('rank_and_file.n.02'), 215), (Synset('achaean.n.02'), 215), (Synset('nation.n.02'), 215), (Synset('business_people.n.01'), 215), (Synset('unconfessed.n.01'), 215), (Synset('chosen_people.n.01'), 215), (Synset('governed.n.01'), 215), (Synset('timid.n.01'), 215), (Synset('retreated.n.01'), 215), (Synset('uninitiate.n.01'), 215), (Synset('wounded.n.01'), 215), (Synset('dead.n.01'), 215), (Synset('smart_money.n.03'), 215), (Synset('population.n.01'), 215), (Synset('defeated.n.01'), 215), (Synset('episcopacy.n.01'), 215), (Synset('network_army.n.01'), 215), (Synset('enemy.n.03'), 215)]\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(single_word_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_tree(tree, interesting_chunks):\n",
    "    try:\n",
    "        tree.label()\n",
    "    except AttributeError:\n",
    "        return\n",
    "    else:\n",
    "        if tree.label() == 'NP':  \n",
    "            interesting_chunks.append(tree.leaves())\n",
    "        else:\n",
    "            for child in tree:\n",
    "                parse_tree(child, interesting_chunks)\n",
    "\n",
    "def algo(collection, max_output_len=2000):    \n",
    "    \"\"\" This implementation of the algorithm accepts \n",
    "        both raw text or list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    if isinstance(collection, str):\n",
    "        sentences = to_sents(collection)\n",
    "    elif isinstance(collection, list):\n",
    "        sentences = collection\n",
    "    else:\n",
    "        print(\"collection passed to function must \\\n",
    "              be either a list of sentences or a raw text.\")\n",
    "\n",
    "    grammar = r\"\"\"\n",
    "      NP: {(<DT|PP\\$>?<JJ>?<NN.>)*}\n",
    "          {<NNP>+}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    \n",
    "    interesting_chunks = []\n",
    "    for sentence in sentences:\n",
    "        tree = cp.parse(nltk.pos_tag(to_tokens(sentence)))\n",
    "        parse_tree(tree, interesting_chunks)\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    for chunk in interesting_chunks:\n",
    "        # Ignore single word chunks for now\n",
    "        if len(chunk) > 1:\n",
    "            chunks.append(\" \".join([sent[0] for sent in chunk]))\n",
    "            \n",
    "    freq_terms = run_freq_terms(chunks, 150)\n",
    "    \n",
    "    output_len = 0\n",
    "    output = OrderedDict()\n",
    "    for term in freq_terms:\n",
    "        output_len += len(term[0])\n",
    "        if output_len > max_output_len:\n",
    "            break\n",
    "        else:\n",
    "            output[term[0].lower()] = \"don't matter\"\n",
    "    \n",
    "    return output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intellectual ventures\n",
      "chris crawford\n",
      "public radio international\n",
      "chris crawford's\n",
      "oasis research\n",
      "ira glass\n",
      "this american life\n",
      "the people\n",
      "peter detkin\n",
      "nathan myhrvold\n",
      "other words\n",
      "silicon valley\n",
      "mike daisey\n",
      "other people\n",
      "alex blumberg\n",
      "these people\n",
      "adrian schoolcraft\n",
      "the patents\n",
      "the numbers\n",
      "tom ewing\n",
      "these patents\n",
      "graham rayman\n",
      "the scrambler\n",
      "the cali\n",
      "julian vinegas\n",
      "the experiments\n",
      "jack byrd's\n",
      "the agents\n",
      "san francisco\n",
      "jack byrd\n",
      "all kinds\n",
      "the cells\n",
      "wbez chicago\n",
      "the dea\n",
      "new york\n",
      "it's this american life\n",
      "the workers\n",
      "kwon holdings\n",
      "the girls\n",
      "many people\n",
      "\"i don't\n",
      "seth lind\n",
      "chuck campos\n",
      "julie snyder\n",
      "the companies\n",
      "nancy updike\n",
      "prime numbers\n",
      "the boys\n",
      "chicago public radio\n",
      "laura sydell\n",
      "michael smith\n",
      "senate bill\n",
      "the details\n",
      "fight club\n",
      "ben calhoun\n",
      "chris sacca\n",
      "the officers\n",
      "many times\n",
      "these pictures\n",
      "the ones\n",
      "these guys\n",
      "sarah koenig\n",
      "the streets\n",
      "no employees\n",
      "the things\n",
      "those patents\n",
      "the guards\n",
      "the machines\n",
      "jonathan menjivar\n",
      "ian spaulding\n",
      "those things\n",
      "alissa shipp\n",
      "internal affairs\n",
      "the data\n",
      "robyn semien\n",
      "mary archbold\n",
      "lisa pollak\n",
      "jorge salcedo\n",
      "the controls\n",
      "david martin\n",
      "the house\n",
      "the united\n",
      "the cops\n",
      "those companies\n",
      "these assets\n",
      "the oasis\n",
      "torey malatia\n",
      "the years\n",
      "the authorities\n",
      "the gates\n",
      "enhanced software\n",
      "the republican\n",
      "some people\n",
      "the votes\n",
      "the constitution\n",
      "a few days\n",
      "brian reed\n",
      "representative drolet\n",
      "new york city\n",
      "electromagnetic waves\n"
     ]
    }
   ],
   "source": [
    "tal_text = open(\"../tal_stories/tal_text_clean.txt\")\n",
    "txt = tal_text.read()\n",
    "tal_text.close()\n",
    "output = algo(txt)\n",
    "for line in output:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the united\n",
      "the president\n",
      "the congo\n",
      "the u.s.\n",
      "president kennedy\n",
      "the house\n",
      "the white house\n",
      "the senate\n",
      "the people\n",
      "the legislature\n",
      "new york\n",
      "the yankees\n",
      "the university\n",
      "premier khrushchev\n",
      "the anti-trust laws\n",
      "the u. s.\n",
      "the soviet\n",
      "san francisco\n",
      "the u.n.\n",
      "the american\n",
      "the rev\n",
      "los angeles\n",
      "new orleans\n",
      "the kennedy\n",
      "the sales\n",
      "the children\n",
      "el paso\n",
      "ap )\n",
      "the members\n",
      "new jersey\n",
      "kansas city\n",
      "american catholic\n",
      "the soviet union\n",
      "the communists\n",
      "the rules committee\n",
      "the president's\n",
      "the army\n",
      "the birds\n",
      "the hughes\n",
      "the students\n",
      "the defendants\n",
      "hong kong\n",
      "arnold palmer\n",
      "the years\n",
      "the orioles\n",
      "the dreadnought\n",
      "last saturday\n",
      "the beardens\n",
      "emory university\n",
      "the masters\n",
      "the republican\n",
      "white house\n",
      "the problems\n",
      "the bible\n",
      "the anti-monopoly laws\n",
      "new york city\n",
      "president-elect kennedy\n",
      "dental schools\n",
      "the puppets\n",
      "dallas county\n",
      "the laws\n",
      "the teamsters\n",
      "the st\n",
      "the children's\n",
      "the government\n",
      "the toll-road bonds\n",
      "mickey mantle\n",
      "east providence\n",
      "rhode island\n",
      "the kremlin\n",
      "the state's\n",
      "pennsylvania avenue\n",
      "the schools\n",
      "capitol hill\n",
      "southeast asia\n",
      "the dallas\n",
      "the communist\n",
      "the bears\n",
      "nuclear weapons\n",
      "beverly hills\n",
      "the morton foods\n",
      "the west\n",
      "the turnpikes\n",
      "toll-road bonds\n",
      "the american league\n",
      "president kennedy's\n",
      "the administration\n",
      "the congolese\n",
      "the citizens group\n",
      "la dolce vita\n",
      "north plains\n",
      "federal grants\n",
      "the builders\n",
      "the meyner\n",
      "the german\n",
      "the words\n",
      "the catholic\n"
     ]
    }
   ],
   "source": [
    "brown = []\n",
    "for sent in nltk.corpus.brown.sents(categories=\"news\"):\n",
    "    brown.append(\" \".join(sent))\n",
    "\n",
    "output = algo(brown)\n",
    "for line in output:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "the characters\n",
      "the special effects\n",
      "the \"\n",
      "special effects\n",
      "the movie's\n",
      "the movies\n",
      "the musketeers\n",
      "the dogs\n",
      "the filmmakers\n",
      "those films\n",
      "the girls\n",
      "the producers\n",
      "the bugs\n",
      "the kids\n",
      "the scenes\n",
      "the games\n",
      "bad guys\n",
      "the film's\n",
      "signs wonders\n",
      "the others\n",
      "the actors\n",
      "the writers\n",
      "some movies\n",
      "the performances\n",
      "many characters\n",
      "many times\n",
      "dramatic contrivances\n",
      "the lambs\n",
      "the streets\n",
      "sports films\n",
      "popular kids\n",
      "the films\n",
      "the words\n",
      "the replacements\n",
      "no points\n",
      "a sports\n",
      "the fights\n",
      "the english\n",
      "the authorities\n",
      "the apes\n",
      "the theaters\n",
      "these people\n",
      "the cops\n",
      "different perspectives\n",
      "the jokes\n",
      "big john's\n",
      "these moments\n",
      "many things\n",
      "nervous smiles\n",
      "the bowels\n",
      "the adventures\n",
      "the early days\n",
      "shelton's films\n",
      "a sci-fi\n",
      "the piano's braces\n",
      "the mortals\n",
      "the ramones\n",
      "the editors\n",
      "the secrets\n",
      "automatic weapons\n",
      "life's riddles\n",
      "who's films\n",
      "sonny chiba's\n",
      "own parallels\n",
      "hot reviews\n",
      "these things\n",
      "the shelves\n",
      "unexpected events\n",
      "other things\n",
      "various ideas\n",
      "bird's-eye-view shots\n",
      "the residents\n",
      "ryan tries\n",
      "the few people\n",
      "some martial arts\n",
      "physical attributes\n",
      "literal snapshots\n",
      "own chords\n",
      "richelieu's man-in-black\n",
      "no bounds\n",
      "the cloud-choked skies\n",
      "paper-thin characters\n",
      "the andrews sisters\n",
      "supernatural chills\n",
      "credible people\n",
      "kong films\n",
      "many reviewers\n",
      "other powers\n",
      "the things\n",
      "diabolical boss\n",
      "the same paces\n",
      "other distributors\n",
      "the tables\n",
      "brief bios\n",
      "nerdy aldys\n",
      "the problems\n",
      "the heroes\n",
      "white men\n",
      "middle-class houses\n",
      "the dodgers\n",
      "anti-climactic endings\n",
      "any major awards\n",
      "the limitations\n",
      "the high schoolers\n",
      "grant's nervous smiles\n",
      "music plays\n",
      "the ads\n",
      "the mars\n",
      "the tv2\n",
      "matthew lillard's\n",
      "amiable leads\n",
      "physical gyrations\n",
      "mann's forensics\n",
      "the scriptwriters\n",
      "jet's fights\n",
      "the other workers\n",
      "the hoosiers\n",
      "the writer's\n",
      "the soldiers\n",
      "this genre's films\n",
      "both cuts\n",
      "the stops\n",
      "the women\n",
      "the talents\n",
      "the matchmaker's\n",
      "the royal musketeers\n",
      "the filmmaker's pornographic proclivities\n",
      "tight situations\n",
      "religious overtones\n",
      "dull sub-plots\n",
      "the main characters\n",
      "those chinese films\n",
      "the cardinal hunts\n",
      "thuggish co-horts\n",
      "these credits\n",
      "strong vehicles\n",
      "the realms\n",
      "a few weeks\n",
      "modern manners\n",
      "these young people\n",
      "all stalkers\n",
      "those disappointing returns\n",
      "stan winston's\n",
      "the makers\n",
      "later it's\n",
      "the snippets\n",
      "film's shrug-of-the-shoulders\n",
      "perth amboys\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "# Mystery Text URL goes here\n",
    "# url = \"\"\n",
    "\n",
    "# page_text = []\n",
    "# urlretrieve(url, \"mystery_expository.txt\")\n",
    "\n",
    "mystery_text = open(\"mystery_expository.txt\")\n",
    "txt = mystery_text.read()\n",
    "mystery_text.close()\n",
    "\n",
    "output = algo(txt)\n",
    "\n",
    "for line in output:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shere khan\n",
      "kala nag\n",
      "the pack\n",
      "little toomai\n",
      "father wolf\n",
      "the monkeys\n",
      "petersen sahib\n",
      "the jungle\n",
      "the elephants\n",
      "gray brother\n",
      "mother wolf\n",
      "the law\n",
      "big toomai\n",
      "the men\n",
      "the council rock\n",
      "the buffaloes\n",
      "the wolves\n",
      "the branches\n",
      "kala nag's\n",
      "\"y es\n",
      "the terms\n",
      "the trees\n",
      "the bullocks\n",
      "sea catch\n",
      "the seals\n",
      "the walls\n",
      "the bulls\n",
      "the hills\n",
      "shere khan's\n",
      "the project gutenberg-tm\n",
      "the cubs\n",
      "machua appa\n",
      "project gutenberg-tm\n",
      "the united\n",
      "little brother\n",
      "the waingunga\n",
      "project gutenberg-tm electronic works\n",
      "the project gutenberg literary archive foundation\n",
      "sea vitch\n",
      "the council\n",
      "the beaches\n",
      "the free people\n",
      "the lines\n",
      "the others\n",
      "the villagers\n",
      "the guns\n",
      "the keddah\n",
      "petersen sahib's\n",
      "the cold lairs\n",
      "the black panther\n",
      "the eyes\n",
      "the red flower\n",
      "the sides\n",
      "sea cow\n",
      "the beasts\n",
      "\"t hat's\n",
      "the pacific\n",
      "a project gutenberg-tm\n",
      "the plains\n",
      "the bear\n",
      "white men\n",
      "the tree-tops\n",
      "electronic works\n",
      "the ways\n",
      "the amir\n",
      "the jungles\n",
      "the young wolves\n",
      "the cows\n",
      "the viceroy\n",
      "mang the bat\n",
      "the people\n",
      "shiva the preserver\n",
      "the jungle .\"\n",
      "the foundation\n",
      "the panther\n",
      "the marks\n",
      "the nurseries\n",
      "gutenberg \"\n",
      "the camels\n",
      "this ebook\n",
      "the stars\n",
      "bad dreams\n",
      "the orders\n",
      "\"h e's\n",
      "the children\n",
      "walrus islet\n",
      "the government\n",
      "the birds\n",
      "these things\n",
      "the bull\n",
      "the sambhur\n",
      "i. \"\n",
      "derivative works\n",
      "the monkey people\n",
      "the things\n",
      "the herds\n",
      "the winds\n",
      "a keddah\n",
      "the new elephants\n",
      "the drivers\n",
      "the waves\n",
      "akela .\"\n",
      "the words\n",
      "the master words\n",
      "the full project gutenberg-tm license\n",
      "hathi the wild elephant\n",
      "all things\n",
      "the man-cub\n",
      "many things\n",
      "o wolves\n",
      "o wolves \"\n",
      "the cliffs\n",
      "the tops\n",
      "nag .)\n",
      "the babies\n",
      "kaa .\"\n",
      "the stones\n",
      "the jungle people\n",
      "the stables\n",
      "\"w hat's\n",
      "mother wolf's\n",
      "any project gutenberg-tm\n",
      "old seals\n",
      "the garo\n",
      "the breakers\n",
      "wild elephants\n",
      "these people\n",
      "the big guns\n",
      "the man's\n",
      "the king\n",
      "the upper branches\n",
      "the days\n",
      "the rocks\n",
      "the only people\n",
      "the laws\n",
      "the hollows\n",
      "sea lion's neck\n",
      "dundee \"\n",
      "a few minutes\n",
      "all references\n",
      "the tanks\n",
      "the full terms\n",
      "the lone wolf\n",
      "the other seals\n",
      "the frog\n",
      "the troops\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "# Mystery Text URL goes here\n",
    "# url = \"\"\n",
    "\n",
    "# page_text = []\n",
    "# urlretrieve(url, \"mystery_expository.txt\")\n",
    "\n",
    "mystery_text = open(\"mystery_text_narrative.txt\")\n",
    "txt = mystery_text.read()\n",
    "mystery_text.close()\n",
    "\n",
    "output = algo(txt)\n",
    "\n",
    "for line in output:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
