{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Categorize Terms With WordNet##\n",
    "Original code by Anna Swigart, MIMS 2015\n",
    "\n",
    "Modified by Marti Hearst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Preliminaries: code to train tagger.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training on all brown sentences, excluding news corpus\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance',\n",
    "'science_fiction'])\n",
    "\n",
    "cooking_action_sents = [[('Strain', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Mix', 'VB'), ('them', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Season', 'VB'), ('them', 'PPS'), ('with', 'IN'), ('pepper', 'NN'), ('.', '.')], \n",
    "                        [('Wash', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Chop', 'VB'), ('the', 'AT'), ('greens', 'NNS'), ('.', '.')],\n",
    "                        [('Slice', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Bake', 'VB'), ('the', 'AT'), ('cake', 'NN'), ('.', '.')],\n",
    "                        [('Pour', 'VB'), ('into', 'IN'), ('a', 'AT'), ('mold', 'NN'), ('.', '.')],\n",
    "                        [('Stir', 'VB'), ('the', 'AT'), ('mixture', 'NN'), ('.', '.')],\n",
    "                        [('Moisten', 'VB'), ('the', 'AT'), ('grains', 'NNS'), ('.', '.')],\n",
    "                        [('Cook', 'VB'), ('the', 'AT'), ('duck', 'NN'), ('.', '.')],\n",
    "                        [('Drain', 'VB'), ('for', 'IN'), ('one', 'CD'), ('day', 'NN'), ('.', '.')]]\n",
    "\n",
    "all_tagged_sents = cooking_action_sents + brown_tagged_sents\n",
    "all_tagged_sents\n",
    "\n",
    "def create_data_sets():\n",
    "    size = int(len(all_tagged_sents) * 0.9)\n",
    "    train_sents = all_tagged_sents[:size]\n",
    "    test_sents = all_tagged_sents[size:]\n",
    "    return train_sents, test_sents\n",
    "train_sents, test_sents = create_data_sets()\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "ngram_tagger = build_backoff_tagger(train_sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Preliminaries: code to tokenize and tag text.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_tag_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences \n",
    "    sents = [nltk.word_tokenize(word) for word in raw_sents] # tokenize sentences\n",
    "    tagged_POS_sents = [ngram_tagger.tag(word) for word in sents] # tags sentences\n",
    "    return tagged_POS_sents\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find important terms: Determine which words occur most frequently that are lemmas within WordNet and that are tagged as nouns.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input is a list of (word, tag) pairs and \n",
    "def freq_normed_unigrams(tagged_sents, num_terms=50):\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "        \n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           and word[0] not in punctuation # remove punctuation\n",
    "                           and word[1].startswith('N')]  #retain only nouns \n",
    "   \n",
    "    top_normed_tagpairs = nltk.FreqDist(normed_tagged_words).most_common(num_terms) #get the num_terms most frequent\n",
    "    return [word for (word,count) in top_normed_tagpairs] #extract out the words from the pairs\n",
    "    \n",
    "\n",
    "# input is a list of (word, tag) pairs and \n",
    "def freq_normed_bigrams(tagged_sents, num_terms=50):\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "        \n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           and word[0] not in punctuation # remove punctuation\n",
    "                           and word[1].startswith('N') # retain only nouns \n",
    "                          ]\n",
    "   \n",
    "    top_normed_tagpairs = nltk.FreqDist(normed_tagged_words).most_common(num_terms) #get the num_terms most frequent\n",
    "    return [word for (word,count) in top_normed_tagpairs] #extract out the words from the pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Group the most frequently occuring nouns under their common hypernyms.  Then find those hypernyms that occur most frequently.  This creates a de facto categorization.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categories_from_hypernyms(termlist, num_cats=20):\n",
    "    \n",
    "    hypterms = []\n",
    "    hypterms_dict = defaultdict(list)\n",
    "    for term in termlist:                  # for each term\n",
    "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
    "        for syn in s:                      # for each lemma synset\n",
    "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
    "                hypterms = hypterms + [hyp.name()]      # Extract the hypernym name and add to list\n",
    "                hypterms_dict[hyp.name()].append(term)  # Extract examples and add them to dict\n",
    "                \n",
    "    hypfd = nltk.FreqDist(hypterms)\n",
    "    for (name, count) in hypfd.most_common(num_cats):\n",
    "        print (name, '({0})'.format(count))\n",
    "        print ('\\t', ', '.join(set(hypterms_dict[name])))\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the code in action on the cookbook data: tokenize and tag it, and then compute the categories.  What works well?  What is problematic?###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (5)\n",
      "\t day, time, hour\n",
      "\n",
      "\n",
      "time_unit.n.01 (4)\n",
      "\t day, minute, hour\n",
      "\n",
      "\n",
      "meat.n.01 (3)\n",
      "\t beef, mutton, veal\n",
      "\n",
      "\n",
      "distance.n.01 (3)\n",
      "\t piece, minute, hour\n",
      "\n",
      "\n",
      "time.n.03 (3)\n",
      "\t day, piece, minute\n",
      "\n",
      "\n",
      "flavorer.n.01 (3)\n",
      "\t pepper, herb, salt\n",
      "\n",
      "\n",
      "british_capacity_unit.n.01 (2)\n",
      "\t pint, quart\n",
      "\n",
      "\n",
      "helping.n.01 (2)\n",
      "\t piece, slice\n",
      "\n",
      "\n",
      "united_states_liquid_unit.n.01 (2)\n",
      "\t pint, quart\n",
      "\n",
      "\n",
      "money.n.01 (2)\n",
      "\t bread, sugar\n",
      "\n",
      "\n",
      "avoirdupois_unit.n.01 (2)\n",
      "\t pound, ounce\n",
      "\n",
      "\n",
      "thing.n.12 (2)\n",
      "\t water, piece\n",
      "\n",
      "\n",
      "dish.n.02 (2)\n",
      "\t stew, soup\n",
      "\n",
      "\n",
      "food.n.02 (2)\n",
      "\t butter, meat\n",
      "\n",
      "\n",
      "containerful.n.01 (2)\n",
      "\t jar, dish\n",
      "\n",
      "\n",
      "herb.n.01 (2)\n",
      "\t parsley, carrot\n",
      "\n",
      "\n",
      "attendant.n.01 (2)\n",
      "\t page\n",
      "\n",
      "\n",
      "united_states_dry_unit.n.01 (2)\n",
      "\t pint, quart\n",
      "\n",
      "\n",
      "happening.n.01 (2)\n",
      "\t gravy, fire\n",
      "\n",
      "\n",
      "case.n.01 (2)\n",
      "\t piece, time\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('cookbooks.txt', 'r') as text_file:\n",
    "    cookbooks_corpus = text_file.read()\n",
    "\n",
    "top_terms = freq_normed_unigrams(tokenize_and_tag_text(cookbooks_corpus), 50)\n",
    "\n",
    "categories_from_hypernyms(top_terms, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it on another collection: state of the union.  First, let's look at what files are available.  Then see what output gets produced on some of these speeches.  Compare to each other and to the cooking collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1945-Truman.txt',\n",
       " '1946-Truman.txt',\n",
       " '1947-Truman.txt',\n",
       " '1948-Truman.txt',\n",
       " '1949-Truman.txt',\n",
       " '1950-Truman.txt',\n",
       " '1951-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1954-Eisenhower.txt',\n",
       " '1955-Eisenhower.txt',\n",
       " '1956-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1958-Eisenhower.txt',\n",
       " '1959-Eisenhower.txt',\n",
       " '1960-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1962-Kennedy.txt',\n",
       " '1963-Johnson.txt',\n",
       " '1963-Kennedy.txt',\n",
       " '1964-Johnson.txt',\n",
       " '1965-Johnson-1.txt',\n",
       " '1965-Johnson-2.txt',\n",
       " '1966-Johnson.txt',\n",
       " '1967-Johnson.txt',\n",
       " '1968-Johnson.txt',\n",
       " '1969-Johnson.txt',\n",
       " '1970-Nixon.txt',\n",
       " '1971-Nixon.txt',\n",
       " '1972-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1974-Nixon.txt',\n",
       " '1975-Ford.txt',\n",
       " '1976-Ford.txt',\n",
       " '1977-Ford.txt',\n",
       " '1978-Carter.txt',\n",
       " '1979-Carter.txt',\n",
       " '1980-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1982-Reagan.txt',\n",
       " '1983-Reagan.txt',\n",
       " '1984-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1986-Reagan.txt',\n",
       " '1987-Reagan.txt',\n",
       " '1988-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1990-Bush.txt',\n",
       " '1991-Bush-1.txt',\n",
       " '1991-Bush-2.txt',\n",
       " '1992-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1994-Clinton.txt',\n",
       " '1995-Clinton.txt',\n",
       " '1996-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '1998-Clinton.txt',\n",
       " '1999-Clinton.txt',\n",
       " '2000-Clinton.txt',\n",
       " '2001-GWBush-1.txt',\n",
       " '2001-GWBush-2.txt',\n",
       " '2002-GWBush.txt',\n",
       " '2003-GWBush.txt',\n",
       " '2004-GWBush.txt',\n",
       " '2005-GWBush.txt',\n",
       " '2006-GWBush.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "state_union.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a SOTU file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (5)\n",
      "\t year, time\n",
      "\n",
      "\n",
      "group.n.01 (5)\n",
      "\t world, people, system\n",
      "\n",
      "\n",
      "people.n.01 (4)\n",
      "\t country, business, nation, world\n",
      "\n",
      "\n",
      "political_unit.n.01 (3)\n",
      "\t country, nation, state\n",
      "\n",
      "\n",
      "action.n.01 (3)\n",
      "\t thing, economy, change\n",
      "\n",
      "\n",
      "attribute.n.02 (3)\n",
      "\t thing, time, state\n",
      "\n",
      "\n",
      "work.n.01 (3)\n",
      "\t job, care\n",
      "\n",
      "\n",
      "person.n.01 (3)\n",
      "\t worker, child\n",
      "\n",
      "\n",
      "administrative_district.n.01 (3)\n",
      "\t country, state\n",
      "\n",
      "\n",
      "activity.n.01 (3)\n",
      "\t business, work, job\n",
      "\n",
      "\n",
      "share.n.01 (3)\n",
      "\t way, interest, cut\n",
      "\n",
      "\n",
      "stroke.n.01 (2)\n",
      "\t cut\n",
      "\n",
      "\n",
      "cash.n.01 (2)\n",
      "\t change\n",
      "\n",
      "\n",
      "idea.n.01 (2)\n",
      "\t program, plan\n",
      "\n",
      "\n",
      "division.n.03 (2)\n",
      "\t cut\n",
      "\n",
      "\n",
      "happening.n.01 (2)\n",
      "\t thing, change\n",
      "\n",
      "\n",
      "system.n.04 (2)\n",
      "\t program, government\n",
      "\n",
      "\n",
      "aim.n.02 (2)\n",
      "\t business, thing\n",
      "\n",
      "\n",
      "concern.n.01 (2)\n",
      "\t thing, world\n",
      "\n",
      "\n",
      "proportion.n.01 (2)\n",
      "\t percent, rate\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sotu_raw_text = state_union.raw(\"1993-Clinton.txt\")\n",
    "top_terms = freq_normed_unigrams(tokenize_and_tag_text(sotu_raw_text), 50)\n",
    "\n",
    "categories_from_hypernyms(top_terms, 20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try another SOTU file here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your collection here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (14)\n",
      "\t day, time, week, month, year, life, hour\n",
      "\n",
      "\n",
      "person.n.01 (6)\n",
      "\t intellectual, friend, case, life\n",
      "\n",
      "\n",
      "time_unit.n.01 (4)\n",
      "\t day, month, hour\n",
      "\n",
      "\n",
      "being.n.01 (3)\n",
      "\t life\n",
      "\n",
      "\n",
      "aim.n.02 (3)\n",
      "\t business, thing, point\n",
      "\n",
      "\n",
      "group.n.01 (3)\n",
      "\t people\n",
      "\n",
      "\n",
      "family.n.04 (2)\n",
      "\t name, people\n",
      "\n",
      "\n",
      "category.n.02 (2)\n",
      "\t way, kind\n",
      "\n",
      "\n",
      "state.n.02 (2)\n",
      "\t office, point\n",
      "\n",
      "\n",
      "attribute.n.02 (2)\n",
      "\t thing, time\n",
      "\n",
      "\n",
      "work_time.n.01 (2)\n",
      "\t day, week\n",
      "\n",
      "\n",
      "artifact.n.01 (2)\n",
      "\t way, thing\n",
      "\n",
      "\n",
      "characteristic.n.02 (2)\n",
      "\t point\n",
      "\n",
      "\n",
      "language_unit.n.01 (2)\n",
      "\t phone, name\n",
      "\n",
      "\n",
      "happening.n.01 (2)\n",
      "\t thing, case\n",
      "\n",
      "\n",
      "tract.n.01 (2)\n",
      "\t lot, oasis\n",
      "\n",
      "\n",
      "grammatical_category.n.01 (2)\n",
      "\t number, case\n",
      "\n",
      "\n",
      "fact.n.01 (2)\n",
      "\t case, point\n",
      "\n",
      "\n",
      "collection.n.01 (2)\n",
      "\t lot, hand\n",
      "\n",
      "\n",
      "document.n.01 (2)\n",
      "\t program, patent\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tal = open(\"../tal_stories/tal_text_clean.txt\", \"r\")\n",
    "tal_text = tal.read()\n",
    "tal.close()\n",
    "\n",
    "top_terms = freq_normed_unigrams(tokenize_and_tag_text(tal_text), 50)\n",
    "\n",
    "categories_from_hypernyms(top_terms, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
