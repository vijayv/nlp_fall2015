{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, load in the file or files below.**  First, take a look at your text.  An easy way to get started is to first read it in, and then run it through the sentence tokenizer to divide it up, even if this division is not fully accurate.  You may have to do a bit of work to figure out which will be the \"opening phrase\" that Wolfram Alpha shows.  Below, write the code to read in the text and split it into sentences, and then print out the **opening phrase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I listen to a lot of podcasts.\n"
     ]
    }
   ],
   "source": [
    "tal = open(\"../tal_stories/tal_text.txt\", \"r\")\n",
    "tal_text = tal.read()\n",
    "sentences = sent_tokenizer.tokenize(tal_text)\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, tokenize.**  Look at the several dozen sentences to see what kind of tokenization issues you'll have.  Write a regular expression tokenizer, using the nltk.regexp_tokenize() as seen in class, or using something more sophisticated if you prefer, to do a nice job of breaking your text up into words.  You may need to make changes to the regex pattern that is given in the book to make it work well for your text collection. \n",
    "\n",
    "*How you break up the words will have effects down the line for how you can manipulate your text collection.  You may want to refine this code later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'EPISODE', 'NUMBER', ':', '496', '>', '<', 'EPISODE', 'NAME', ':']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "    <\n",
    "    | :\n",
    "    | ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "   | \\w+([-']\\w+)*        # words with optional internal hyphens\n",
    "   | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "   | \\.\\.\\.            # ellipsis\n",
    "   | [.,;\"'?():-_`]+  # these are separate tokens\n",
    " '''\n",
    "tokens = nltk.regexp_tokenize(tal_text,pattern)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute word counts.** Now compute your frequency distribution using a FreqDist over the words. Let's not do lowercasing or stemming yet.  You can run this over the whole collection together, or sentence by sentence. Write the code for computing the FreqDist below and show the most common 20 words that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 5963),\n",
       " (',', 5220),\n",
       " ('the', 3293),\n",
       " ('to', 2313),\n",
       " ('a', 1966),\n",
       " ('of', 1710),\n",
       " (':', 1689),\n",
       " ('>', 1665),\n",
       " ('<', 1665),\n",
       " ('I', 1551),\n",
       " ('that', 1432),\n",
       " ('and', 1420),\n",
       " ('in', 1197),\n",
       " ('And', 1162),\n",
       " ('you', 965),\n",
       " ('was', 887),\n",
       " ('SUBJECT', 818),\n",
       " ('it', 806),\n",
       " ('is', 784),\n",
       " ('he', 679)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(tokens)\n",
    "fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the text.** Now adjust the output by normalizing the text: things you can try include removing stopwords, removing very short words, lowercasing the text, improving the tokenization, and/or doing other adjustments to bring content words higher up in the results.  The goal is to dig deeper into the collection to find interesting but relatively frequent words.  Show the code for these changes below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "exclude = list(string.punctuation) + list(string.digits) + ['...'] + stopwords.words('english')\n",
    "filtered_tokens = list(filter(lambda x: (not(x.isupper()) and not(x in exclude) and len(x) > 5), tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show adjusted word counts.** Show the most frequent 20 words that result from these adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', 300),\n",
       " ('patent', 227),\n",
       " ('really', 149),\n",
       " ('Ventures', 120),\n",
       " ('Intellectual', 120),\n",
       " ('actually', 118),\n",
       " (\"didn't\", 111),\n",
       " ('company', 111),\n",
       " ('patents', 109),\n",
       " ('something', 106),\n",
       " ('things', 105),\n",
       " ('called', 102),\n",
       " (\"that's\", 101),\n",
       " ('companies', 101),\n",
       " (\"you're\", 99),\n",
       " ('little', 90),\n",
       " (\"That's\", 83),\n",
       " ('number', 81),\n",
       " (\"they're\", 80),\n",
       " ('started', 77)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd2 = nltk.FreqDist(filtered_tokens)\n",
    "fd2_most_common = fd2.most_common(20)\n",
    "fd2_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a table.**\n",
    "Python provides an easy way to line columns up in a table.  You can specify a width for a string such as %6s, producing a string that is padded to width 6. It is right-justified by default, but a minus sign in front of it switches it to left-justified, so -3d% means left justify an integer with width 3.  *AND* if you don't know the width in advance, you can make it a variable by using an asterisk rather than a number before the '\\*s%' or the '-\\*d%'.  Check out this example (this is just fyi):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info type        Value           \n",
      "number of words  100000          \n"
     ]
    }
   ],
   "source": [
    "print('%-16s' % 'Info type', '%-16s' % 'Value')\n",
    "print('%-16s' % 'number of words', '%-16d' % 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Properties Table** Next there is a table of word properties, which you should compute (skip unique word stems, since we will do stemming in class on Wed).  Make a table that prints out:\n",
    "1. number of words\n",
    "2. number of unique words\n",
    "3. average word length\n",
    "4. longest word\n",
    "\n",
    "You can make your table look prettier than the example I showed above if you like!\n",
    "\n",
    "You can decide for yourself if you want to eliminate punctuation and function words (stop words) or not.  It's your collection!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info type        Value           \n",
      "number of words  20444           \n",
      "number of unique words 5385            \n",
      "average word length 7               \n",
      "longest word     cosmetically-altered\n"
     ]
    }
   ],
   "source": [
    "print('%-16s' % 'Info type', '%-16s' % 'Value')\n",
    "print('%-16s' % 'number of words', '%-16d' % len(filtered_tokens))\n",
    "print('%-16s' % 'number of unique words', '%-16d' % len(set(filtered_tokens)))\n",
    "import statistics\n",
    "word_lengths = [len(x) for x in filtered_tokens]\n",
    "print('%-16s' % 'average word length', '%-16d' % round(statistics.mean(word_lengths), 2))\n",
    "print('%-16s' % 'longest word', '%-16s' % max(filtered_tokens, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Frequent Words List.** Next is the most frequent words list.  This table shows the percent of the total as well as the most frequent words, so compute this number as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word             Percent         \n",
      "people           1.47            \n",
      "patent           1.11            \n",
      "really           0.73            \n",
      "Ventures         0.59            \n",
      "Intellectual     0.59            \n",
      "actually         0.58            \n",
      "didn't           0.54            \n",
      "company          0.54            \n",
      "patents          0.53            \n",
      "something        0.52            \n",
      "things           0.51            \n",
      "called           0.5             \n",
      "that's           0.49            \n",
      "companies        0.49            \n",
      "you're           0.48            \n",
      "little           0.44            \n",
      "That's           0.41            \n",
      "number           0.4             \n",
      "they're          0.39            \n",
      "started          0.38            \n"
     ]
    }
   ],
   "source": [
    "count_of_words = len(filtered_tokens)\n",
    "print('%-16s' % 'Word', '%-16s' % 'Percent')\n",
    "for item in fd2_most_common:\n",
    "    print('%-16s' % item[0], '%-16s' % str(round(((item[1]/count_of_words) * 100), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Frequent Capitalized Words List** We haven't lower-cased the text so you should be able to compute this. Don't worry about whether capitalization comes from proper nouns, start of sentences, or elsewhere. You need to make a different FreqDist to do this one.  Write the code here for the new FreqDist and the List itself.  Show the list here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ventures', 120),\n",
       " ('Intellectual', 120),\n",
       " (\"That's\", 83),\n",
       " ('Anthony', 68),\n",
       " ('Because', 61),\n",
       " (\"There's\", 52),\n",
       " (\"They're\", 45),\n",
       " ('Crawford', 43),\n",
       " ('Schoolcraft', 43),\n",
       " ('American', 43),\n",
       " ('Public', 41),\n",
       " ('Adrian', 38),\n",
       " (\"Crawford's\", 37),\n",
       " ('Foxconn', 36),\n",
       " ('International', 31),\n",
       " ('Sadeem', 29),\n",
       " ('Research', 28),\n",
       " ('Detkin', 28),\n",
       " ('Myhrvold', 25),\n",
       " (\"You're\", 25)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude = list(string.punctuation) + list(string.digits) + ['...'] + stopwords.words('english')\n",
    "capitalized_words = list(filter(lambda x: (x[0].isupper() and not(x.isupper()) and not(x in exclude) and len(x) > 5), tokens))\n",
    "fd3 = nltk.FreqDist(capitalized_words)\n",
    "fd3_most_common = fd3.most_common(20)\n",
    "fd3_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Properties Table** This summarizes number of sentences and average sentence length in words and characters (you decide if you want to include stopwords/punctuation or not).  Print those out in a table here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info type        Value           \n",
      "Number of Sentences 6778            \n",
      "Average Length in Words 15              \n",
      "Average Length in Characters 72              \n"
     ]
    }
   ],
   "source": [
    "print('%-16s' % 'Info type', '%-16s' % 'Value')\n",
    "print('%-16s' % 'Number of Sentences', '%-16d' % len(sentences))\n",
    "print('%-16s' % 'Average Length in Words', '%-16d' % statistics.mean([len(nltk.regexp_tokenize(sentence,pattern)) for sentence in sentences]))\n",
    "print('%-16s' % 'Average Length in Characters', '%-16d' % statistics.mean([len(sentence) for sentence in sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflect on the Output** (Write a brief paragraph below answering these questions.) What does it tell you about your collection?  What does it fail to tell you?  How does your collection perhaps differ from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is interesting to see some of the top words from the This American Life Corpus. Since I primarily focused on the episodes that were tagged as technology related, it is interesting to see some of the more hot topic items in the most frequent word lists such as foxcon, patents, and intellectual (property). I think this shows the various topics of technology that a more general audience, such as that of This American Life, may be interested. However, since most of the topics are related to the two episodes related to patents, I may need to think about expanding my subset of episodes that I have chosen to a more diverse set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare to Another Collection** Now do the same analysis on another collection in NLTK.  \n",
    "If your collection is a book, you can compare against another book.   Or you can contrast against an entirely different collection  (Brown corpus, presidential inaugural addresses, etc) to see the difference.\n",
    "The list of collections is here: http://www.nltk.org/nltk_data/\n",
    "Reflect on the similarities to or differences from your text collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_of_union = nltk.corpus.state_union.raw()\n",
    "pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "    <\n",
    "    | :\n",
    "    | ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "   | \\w+([-']\\w+)*        # words with optional internal hyphens\n",
    "   | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "   | \\.\\.\\.            # ellipsis\n",
    "   | [.,;\"'?():-_`]+  # these are separate tokens\n",
    " '''\n",
    "sou_tokens = nltk.regexp_tokenize(state_of_union,pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute word counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 5963),\n",
       " (',', 5220),\n",
       " ('the', 3293),\n",
       " ('to', 2313),\n",
       " ('a', 1966),\n",
       " ('of', 1710),\n",
       " (':', 1689),\n",
       " ('>', 1665),\n",
       " ('<', 1665),\n",
       " ('I', 1551),\n",
       " ('that', 1432),\n",
       " ('and', 1420),\n",
       " ('in', 1197),\n",
       " ('And', 1162),\n",
       " ('you', 965),\n",
       " ('was', 887),\n",
       " ('SUBJECT', 818),\n",
       " ('it', 806),\n",
       " ('is', 784),\n",
       " ('he', 679)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sou_fd = nltk.FreqDist(tokens)\n",
    "sou_fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show Adjusted Word Counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', 1264),\n",
       " ('Congress', 1014),\n",
       " ('America', 880),\n",
       " ('American', 768),\n",
       " ('Federal', 565),\n",
       " ('Americans', 559),\n",
       " ('program', 521),\n",
       " ('economic', 504),\n",
       " ('States', 496),\n",
       " ('Government', 485),\n",
       " ('country', 480),\n",
       " ('United', 474),\n",
       " ('freedom', 457),\n",
       " ('economy', 430),\n",
       " ('national', 429),\n",
       " ('government', 419),\n",
       " ('pplause', 415),\n",
       " ('security', 410),\n",
       " ('nations', 404),\n",
       " ('budget', 403)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude = list(string.punctuation) + list(string.digits) + ['...'] + stopwords.words('english')\n",
    "sou_filtered_tokens = list(filter(lambda x: (not(x.isupper()) and not(x in exclude) and len(x) > 5), sou_tokens))\n",
    "\n",
    "sou_fd2 = nltk.FreqDist(sou_filtered_tokens)\n",
    "sou_fd2_most_common = sou_fd2.most_common(20)\n",
    "sou_fd2_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Properties Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info type        Value           \n",
      "number of words  114601          \n",
      "number of unique words 11777           \n",
      "average word length 8               \n",
      "longest word     competition-restricting\n"
     ]
    }
   ],
   "source": [
    "print('%-16s' % 'Info type', '%-16s' % 'Value')\n",
    "print('%-16s' % 'number of words', '%-16d' % len(sou_filtered_tokens))\n",
    "print('%-16s' % 'number of unique words', '%-16d' % len(set(sou_filtered_tokens)))\n",
    "import statistics\n",
    "sou_word_lengths = [len(x) for x in sou_filtered_tokens]\n",
    "print('%-16s' % 'average word length', '%-16d' % round(statistics.mean(sou_word_lengths), 2))\n",
    "print('%-16s' % 'longest word', '%-16s' % max(sou_filtered_tokens, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Frequent Word List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word             Percent         \n",
      "people           1.1             \n",
      "Congress         0.88            \n",
      "America          0.77            \n",
      "American         0.67            \n",
      "Federal          0.49            \n",
      "Americans        0.49            \n",
      "program          0.45            \n",
      "economic         0.44            \n",
      "States           0.43            \n",
      "Government       0.42            \n",
      "country          0.42            \n",
      "United           0.41            \n",
      "freedom          0.4             \n",
      "economy          0.38            \n",
      "national         0.37            \n",
      "government       0.37            \n",
      "pplause          0.36            \n",
      "security         0.36            \n",
      "nations          0.35            \n",
      "budget           0.35            \n"
     ]
    }
   ],
   "source": [
    "count_of_words = len(sou_filtered_tokens)\n",
    "print('%-16s' % 'Word', '%-16s' % 'Percent')\n",
    "for item in sou_fd2_most_common:\n",
    "    print('%-16s' % item[0], '%-16s' % str(round(((item[1]/count_of_words) * 100), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Frequent Capitalized Word List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Congress', 1014),\n",
       " ('America', 880),\n",
       " ('American', 768),\n",
       " ('Federal', 565),\n",
       " ('Americans', 559),\n",
       " ('States', 496),\n",
       " ('Government', 485),\n",
       " ('United', 474),\n",
       " ('President', 347),\n",
       " ('Nation', 285),\n",
       " (\"America's\", 192),\n",
       " ('Soviet', 170),\n",
       " ('Security', 137),\n",
       " ('Europe', 128),\n",
       " ('Tonight', 115),\n",
       " ('Nations', 115),\n",
       " ('Social', 112),\n",
       " ('Members', 100),\n",
       " ('Speaker', 91),\n",
       " ('Administration', 88)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude = list(string.punctuation) + list(string.digits) + ['...'] + stopwords.words('english')\n",
    "sou_capitalized_words = list(filter(lambda x: (x[0].isupper() and not(x.isupper()) and not(x in exclude) and len(x) > 5), sou_tokens))\n",
    "sou_fd3 = nltk.FreqDist(sou_capitalized_words)\n",
    "sou_fd3_most_common = sou_fd3.most_common(20)\n",
    "sou_fd3_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Properties Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only yesterday, we laid to rest the mortal remains of our beloved President, Franklin Delano Roosevelt.\n",
      "Info type        Value           \n",
      "Number of Sentences 17736           \n",
      "Average Length in Words 21              \n",
      "Average Length in Characters 115             \n"
     ]
    }
   ],
   "source": [
    "sou_sentences = sent_tokenizer.tokenize(state_of_union)\n",
    "print(sou_sentences[1])\n",
    "\n",
    "print('%-16s' % 'Info type', '%-16s' % 'Value')\n",
    "print('%-16s' % 'Number of Sentences', '%-16d' % len(sou_sentences))\n",
    "print('%-16s' % 'Average Length in Words', '%-16d' % statistics.mean([len(nltk.regexp_tokenize(sentence,pattern)) for sentence in sou_sentences]))\n",
    "print('%-16s' % 'Average Length in Characters', '%-16d' % statistics.mean([len(sentence) for sentence in sou_sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I picked to compare my collection to the state of the union because they are both transcripts of spoken word. Therefore, I thought it would be interesting to see the similarities and differences betweent the two. Firstly, it is interesting to see that the most frequent word in both corpuses is \"people\". Although, this may have just been due to the fact that I had a minimum character count of 6 or more for words to be included. Secondly, the average count of words per sentence between the two corpuses is vastly different. In the This American Life corpus, the average word count is 14, compared to 21 in the state of the union. This may be due to the fact that This American Life includes dialogue between multiple people and has reponses that would count as a single sentence (such as a guest responding to questions with yes, or no)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
