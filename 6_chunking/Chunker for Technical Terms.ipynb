{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Syntactic Patterns for Technical Terms ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the Manning and Schuetze chapter, there is a well-known part-of-speech \n",
    "based pattern defined by Justeson and Katz for identifying simple noun phrases \n",
    "that often works well for pulling out keyphrases.\n",
    "\n",
    " Technical Term  T = (A | N)+ (N | C)  | N\n",
    "\n",
    "Below, write a function to  define a chunker using the RegexpParser as illustrated in the NLTK book Chapter 7 section 2.3 *Chunking with Regular Expressions*.  You'll need to revise the grammar rules shown there to match the pattern shown above.  You can be liberal with your definition of what is meant by *N* here.  Also, C refers to cardinal number, which is CD in the brown corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (T Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  her/PP$\n",
      "  long/JJ\n",
      "  golden/JJ\n",
      "  (T hair/NN))\n"
     ]
    }
   ],
   "source": [
    "# Technical Term T = (A | N)+ (N | C) | N\n",
    "grammar = r\"\"\"\n",
    "  T: {(<JJ.|NN.*>)+(<NN.*|CD>)|<NN.*>}   # chunk technical term\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), \n",
    "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, write a function to call the chunker, run it on some sentences, and then print out the results for  those sentences.\n",
    "\n",
    "For uniformity, please run it on sentences 100 through 104 from the full tagged brown corpus.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Daniel/NP\n",
      "  personally/RB\n",
      "  led/VBD\n",
      "  the/AT\n",
      "  (T fight/NN)\n",
      "  for/IN\n",
      "  the/AT\n",
      "  (T measure/NN)\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  he/PPS\n",
      "  had/HVD\n",
      "  watered/VBN\n",
      "  down/RP\n",
      "  considerably/RB\n",
      "  since/IN\n",
      "  its/PP$\n",
      "  (T rejection/NN)\n",
      "  by/IN\n",
      "  two/CD\n",
      "  previous/JJ\n",
      "  (T Legislatures/NNS-TL)\n",
      "  ,/,\n",
      "  in/IN\n",
      "  a/AT\n",
      "  public/JJ\n",
      "  (T hearing/NN)\n",
      "  before/IN\n",
      "  the/AT\n",
      "  (T House/NN-TL Committee/NN-TL)\n",
      "  on/IN-TL\n",
      "  (T Revenue/NN-TL)\n",
      "  and/CC-TL\n",
      "  (T Taxation/NN-TL)\n",
      "  ./.)\n",
      "(S\n",
      "  Under/IN\n",
      "  (T committee/NN rules/NNS)\n",
      "  ,/,\n",
      "  it/PPS\n",
      "  went/VBD\n",
      "  automatically/RB\n",
      "  to/IN\n",
      "  a/AT\n",
      "  (T subcommittee/NN)\n",
      "  for/IN\n",
      "  one/CD\n",
      "  (T week/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  But/CC\n",
      "  (T questions/NNS)\n",
      "  with/IN\n",
      "  which/WDT\n",
      "  (T committee/NN members/NNS)\n",
      "  taunted/VBD\n",
      "  (T bankers/NNS)\n",
      "  appearing/VBG\n",
      "  as/CS\n",
      "  (T witnesses/NNS)\n",
      "  left/VBD\n",
      "  little/AP\n",
      "  (T doubt/NN)\n",
      "  that/CS\n",
      "  they/PPSS\n",
      "  will/MD\n",
      "  recommend/VB\n",
      "  (T passage/NN)\n",
      "  of/IN\n",
      "  it/PPO\n",
      "  ./.)\n",
      "(S\n",
      "  Daniel/NP\n",
      "  termed/VBD\n",
      "  ``/``\n",
      "  extremely/RB\n",
      "  conservative/JJ\n",
      "  ''/''\n",
      "  his/PP$\n",
      "  (T estimate/NN)\n",
      "  that/CS\n",
      "  it/PPS\n",
      "  would/MD\n",
      "  produce/VB\n",
      "  17/CD\n",
      "  million/CD\n",
      "  (T dollars/NNS)\n",
      "  to/TO\n",
      "  help/VB\n",
      "  erase/VB\n",
      "  an/AT\n",
      "  anticipated/VBN\n",
      "  (T deficit/NN)\n",
      "  of/IN\n",
      "  63/CD\n",
      "  million/CD\n",
      "  (T dollars/NNS)\n",
      "  at/IN\n",
      "  the/AT\n",
      "  (T end/NN)\n",
      "  of/IN\n",
      "  the/AT\n",
      "  current/JJ\n",
      "  fiscal/JJ\n",
      "  (T year/NN)\n",
      "  next/AP\n",
      "  Aug./NP\n",
      "  31/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "brown_tagged = brown.tagged_sents()\n",
    "for sent in brown_tagged[100:104]:\n",
    "    print(cp.parse(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then extract out the phrases themselves on sentences 100 through 160 using the subtree extraction technique shown in the \n",
    "*Exploring Text Corpora* category.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(T fight/NN)\n",
      "(T measure/NN)\n",
      "(T rejection/NN)\n",
      "(T Legislatures/NNS-TL)\n",
      "(T hearing/NN)\n",
      "(T House/NN-TL Committee/NN-TL)\n",
      "(T Revenue/NN-TL)\n",
      "(T Taxation/NN-TL)\n",
      "(T committee/NN rules/NNS)\n",
      "(T subcommittee/NN)\n",
      "(T week/NN)\n",
      "(T questions/NNS)\n",
      "(T committee/NN members/NNS)\n",
      "(T bankers/NNS)\n",
      "(T witnesses/NNS)\n",
      "(T doubt/NN)\n",
      "(T passage/NN)\n",
      "(T estimate/NN)\n",
      "(T dollars/NNS)\n",
      "(T deficit/NN)\n",
      "(T dollars/NNS)\n",
      "(T end/NN)\n",
      "(T year/NN)\n",
      "(T committee/NN)\n",
      "(T measure/NN)\n",
      "(T means/NNS)\n",
      "(T escheat/NN law/NN)\n",
      "(T books/NNS)\n",
      "(T republic/NN)\n",
      "(T state/NN)\n",
      "(T bank/NN accounts/NNS)\n",
      "(T stocks/NNS)\n",
      "(T property/NN)\n",
      "(T persons/NNS)\n",
      "(T years/NNS)\n",
      "(T bill/NN)\n",
      "(T banks/NNS)\n",
      "(T insurance/NN firms/NNS)\n",
      "(T pipeline/NN companies/NNS)\n",
      "(T corporations/NNS)\n",
      "(T property/NN)\n",
      "(T state/NN treasurer/NN)\n",
      "(T escheat/NN law/NN)\n",
      "(T property/NN)\n",
      "(T lawyer/NN)\n",
      "(T Bankers/NNS-TL Association/NN-TL)\n",
      "(T opposition/NN keynote/NN)\n",
      "(T banks/NNS)\n",
      "(T obligations/NNS)\n",
      "(T depositors/NNS)\n",
      "(T confidence/NN)\n",
      "(T bank/NN customers/NNS)\n",
      "(T confidence/NN)\n",
      "(T banks/NNS)\n",
      "(T economy/NN)\n",
      "(T circulation/NN)\n",
      "(T millions/NNS)\n",
      "(T dollars/NNS)\n",
      "(T Rep./NN-TL)\n",
      "(T sponsor/NN)\n",
      "(T bill/NN)\n",
      "(T failure/NN)\n",
      "(T gift/NN)\n",
      "(T taxpayers'/NNS$ pockets/NNS)\n",
      "(T banks/NNS)\n",
      "(T insurance/NN)\n",
      "(T pipeline/NN companies/NNS)\n",
      "(T contention/NN)\n",
      "(T bankers/NNS)\n",
      "(T bill/NN)\n",
      "(T contracts/NNS)\n",
      "(T notice/NN)\n",
      "(T hearing/NN)\n",
      "(T bill/NN)\n",
      "(T Senators/NNS)\n",
      "(T bill/NN)\n",
      "(T Sen./NN-TL)\n",
      "(T establishment/NN)\n",
      "(T day/NN schools/NNS)\n",
      "(T largest/JJT counties/NNS)\n",
      "(T bill/NN)\n",
      "(T schooling/NN)\n",
      "(T students/NNS)\n",
      "(T age/NN)\n",
      "(T cost/NN)\n",
      "(T state/NN)\n",
      "(T debate/NN)\n",
      "(T Senate/NN-TL)\n",
      "(T bill/NN)\n",
      "(T House/NN-TL)\n",
      "(T Education/NN-TL Agency/NN-TL)\n",
      "(T day/NN schools/NNS)\n",
      "(T counties/NNS)\n",
      "(T population/NN)\n",
      "(T children/NNS)\n",
      "(T years/NNS)\n",
      "(T age/NN)\n",
      "(T day/NN schools/NNS)\n",
      "(T older/JJR ones/NNS)\n",
      "(T School/NN-TL)\n",
      "(T budget/NN)\n",
      "(T day/NN schools/NNS)\n",
      "(T counties/NNS)\n",
      "(T $451,500/NNS)\n",
      "(T savings/NNS)\n",
      "(T $157,460/NNS)\n",
      "(T year's/NN$ capital/NN outlay/NN)\n",
      "(T $88,000/NNS)\n",
      "(T Senate/NN-TL)\n",
      "(T TEA/NN)\n",
      "(T scholastics/NNS)\n",
      "(T day/NN school/NN)\n",
      "(T County/NN-TL)\n",
      "(T state/NN)\n",
      "(T school/NN)\n",
      "(T debate/NN)\n",
      "(T horse/NN race/NN parimutuels/NNS)\n",
      "(T Reps./NNS-TL)\n",
      "(T details/NNS)\n",
      "(T folks/NNS)\n",
      "(T amendment/NN)\n",
      "(T letters/NNS)\n",
      "(T horse/NN race/NN betting/NN)\n",
      "(T people/NNS)\n",
      "(T question/NN)\n",
      "(T career/NN)\n",
      "(T Rep./NN-TL)\n",
      "(T ex-gambler/NN)\n",
      "(T advocacy/NN)\n",
      "(T ponies/NNS)\n",
      "(T House/NN-TL committee/NN)\n",
      "(T option/NN proposal/NN)\n",
      "(T report/NN)\n",
      "(T resolution/NN)\n",
      "(T sledding/NN)\n",
      "(T house/NN)\n",
      "(T Senate/NN-TL)\n",
      "(T bill/NN)\n",
      "(T State/NN-TL Health/NN-TL Department's/NN$-TL authority/NN)\n",
      "(T assistance/NN)\n",
      "(T cities/NNS)\n",
      "(T senate/NN)\n",
      "(T fare/NN)\n",
      "(T House/NN-TL bills/NNS)\n",
      "(T committees/NNS)\n",
      "(T calendar/NN)\n",
      "(T acts/NNS)\n",
      "(T school/NN districts/NNS)\n",
      "(T authority/NN)\n",
      "(T Navigation/NN-TL District/NN-TL)\n",
      "(T act/NN)\n",
      "(T creation/NN)\n",
      "(T county/NN-TL Hospital/NN-TL District/NN-TL)\n",
      "(T amendment/NN)\n",
      "(T dissent/NN)\n",
      "(T senators/NNS)\n",
      "(T bill/NN)\n",
      "(T Sen./NN-TL)\n",
      "(T establishment/NN)\n",
      "(T future/NN)\n",
      "(T school/NN)\n",
      "(T Gulf/NN-TL Coast/NN-TL district/NN)\n",
      "(T Money/NN)\n",
      "(T construction/NN)\n",
      "(T meantime/NN)\n",
      "(T State/NN-TL Hospital/NN-TL board/NN)\n",
      "(T gifts/NNS)\n",
      "(T donations/NNS)\n",
      "(T site/NN)\n",
      "(T tax/NN revision/NN bills/NNS)\n",
      "(T Sen./NN-TL)\n",
      "(T retailers/NNS)\n",
      "(T group/NN)\n",
      "(T excise/NN taxes/NNS)\n",
      "(T requirement/NN)\n",
      "(T return/NN)\n",
      "(T retailers/NNS)\n",
      "(T certificate/NN)\n",
      "(T correctness/NN)\n",
      "(T violation/NN)\n",
      "(T penalty/NN)\n",
      "(T years/NNS)\n",
      "(T prison/NN)\n",
      "(T $1,000/NNS fine/NN)\n",
      "(T series/NN)\n",
      "(T recommendations/NNS)\n",
      "(T Research/NN-TL League/NN-TL)\n",
      "(T bill/NN)\n",
      "(T Sen./NN-TL)\n",
      "(T estate/NN brokers/NNS)\n",
      "(T fee/NN)\n",
      "(T $12/NNS)\n",
      "(T occupation/NN license/NN)\n",
      "(T brokers/NNS)\n",
      "(T stocks/NNS)\n",
      "(T bonds/NNS)\n",
      "(T gas/NN)\n",
      "(T utility/NN companies/NNS)\n",
      "(T right/NN)\n",
      "(T domain/NN)\n",
      "(T bill/NN)\n",
      "(T Sen./NN-TL)\n",
      "(T sites/NNS)\n",
      "(T storage/NN reservoirs/NNS)\n",
      "(T gas/NN)\n",
      "(T chairman/NN)\n",
      "(T Highway/NN-TL Commission/NN-TL)\n",
      "(T plan/NN)\n",
      "(T appointment/NN)\n",
      "(T vacancies/NNS)\n",
      "(T Legislature/NN-TL)\n",
      "(T need/NN)\n",
      "(T elections/NNS)\n",
      "(T plan/NN)\n",
      "(T appointee/NN)\n",
      "(T board/NN)\n",
      "(T governor/NN)\n",
      "(T lieutenant/NN governor/NN)\n",
      "(T speaker/NN)\n",
      "(T House/NN-TL)\n",
      "(T attorney/NN general/NN)\n",
      "(T chief/JJS justice/NN)\n",
      "(T Court/NN-TL)\n",
      "(T State/NN representatives/NNS)\n",
      "(T poll/NN)\n",
      "(T kind/NN)\n",
      "(T taxes/NNS)\n",
      "(T vote/NN)\n",
      "(T State/NN-TL Affairs/NNS-TL Committee/NN-TL)\n",
      "(T bill/NN)\n",
      "(T referendum/NN)\n",
      "(T ballot/NN)\n",
      "(T senator/NN)\n",
      "(T Rep./NN-TL)\n",
      "(T sponsor/NN)\n",
      "(T poll/NN idea/NN)\n",
      "(T further/JJR delay/NN)\n",
      "(T committee/NN)\n",
      "(T bill/NN)\n",
      "(T Chairman/NN-TL)\n",
      "(T committee/NN)\n",
      "(T hearing/NN)\n",
      "(T proposal/NN)\n",
      "(T approval/NN)\n",
      "(T two-thirds/NNS majorities/NNS)\n",
      "(T ballot/NN)\n",
      "(T Opponents/NNS)\n",
      "(T ballot/NN)\n",
      "(T information/NN)\n",
      "(T tax/NN proposals/NNS)\n",
      "(T voters/NNS)\n",
      "(T choice/NN)\n",
      "(T members/NNS)\n",
      "(T Rep./NN-TL)\n",
      "(T Paradise/NN-TL)\n",
      "(T water/NN needs/NNS)\n",
      "(T cities/NNS)\n",
      "(T Rep./NN-TL)\n",
      "(T water/NN development/NN bill/NN)\n",
      "(T House/NN-TL)\n",
      "(T Representatives/NNS-TL)\n",
      "(T effort/NN)\n",
      "(T cities/NNS)\n",
      "(T Fort/NN-TL)\n",
      "(T places/NNS)\n",
      "(T Paradise/NN-TL)\n",
      "(T County/NN-TL hamlet/NN)\n",
      "(T people/NNS)\n",
      "(T shouting/NN)\n",
      "(T bill/NN)\n",
      "(T Senate/NN-TL)\n",
      "(T proposal/NN)\n",
      "(T Sen./NN-TL)\n",
      "(T fire/NN)\n",
      "(T Sen./NN-TL)\n",
      "(T bill/NN)\n",
      "(T $5,000,000/NNS)\n",
      "(T $15,000,000/NNS)\n",
      "(T loan/NN)\n",
      "(T state/NN)\n",
      "(T water/NN project/NN)\n",
      "(T effort/NN)\n",
      "(T cities/NNS)\n",
      "(T money/NN)\n",
      "(T water/NN)\n",
      "(T Statements/NNS)\n",
      "(T legislators/NNS)\n",
      "(T water/NN program/NN)\n",
      "(T bonds/NNS)\n",
      "(T places/NNS)\n",
      "(T bill/NN)\n",
      "(T attack/NN)\n"
     ]
    }
   ],
   "source": [
    "for sent in brown_tagged[100:160]:\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'T': print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Proper Nouns ##\n",
    "For this next task, write a new version of the chunker, but this time change it in two ways:\n",
    " 1. Make it recognize proper nouns\n",
    " 2. Make it work on your personal text collection which means that you need to run a tagger over your personal text collection.\n",
    "\n",
    "Note that the second requirements means that you need to run a tagger over your personal text collection before you design the proper noun recognizer.  You can use a pre-trained tagger or train your own on one of the existing tagged collections (brown, conll, or treebank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<EPISODE NUMBER:496> <EPISODE NAME:When Patents Attack... Part Two!> <HOST:IRA GLASS>  OK. I listen '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tal = open(\"../tal_stories/tal_text.txt\", \"r\")\n",
    "tal_text = tal.read()\n",
    "\n",
    "tal.close()\n",
    "# remove tags\n",
    "\n",
    "tal_text[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sents = sent_tokenizer.tokenize(tal_text)\n",
    "tal_sents = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "tal_sents[100:103]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tagger:** Your code for optionally training tagger, and for definitely running tagger on your personal collection goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I', 'PPSS'), ('listen', 'VB'), ('to', 'TO'), ('a', 'AT'), ('lot', 'NN'), ('of', 'IN'), ('podcasts', 'NN'), ('.', '.')], [('I', 'PPSS'), ('host', 'NN'), ('a', 'AT'), ('radio', 'NN'), ('show', 'NN'), ('that', 'WPS'), ('is', 'BEZ'), ('distributed', 'VBN'), ('as', 'CS'), ('a', 'AT'), ('podcast', 'NN'), ('.', '.')], [('So', 'RB'), ('I', 'PPSS'), ('had', 'HVD'), ('a', 'AT'), ('special', 'JJ'), ('interest', 'NN'), ('in', 'IN'), ('some', 'DTI'), ('interviews', 'NNS'), ('that', 'WPS'), ('one', 'CD'), ('of', 'IN'), ('my', 'PP$'), ('colleagues', 'NNS'), (',', ','), ('Zoe', 'NP'), ('Chace', 'NN'), (',', ','), ('did', 'DOD'), ('recently', 'RB'), ('.', '.')], [('She', 'PPS'), (\"'s\", 'NN'), ('one', 'CD'), ('of', 'IN'), ('the', 'AT'), ('reporters', 'NNS'), ('for', 'IN'), ('Planet', 'NN'), ('Money', 'NN'), (',', ','), ('which', 'WDT'), ('itself', 'PPL'), ('is', 'BEZ'), ('a', 'AT'), ('podcast', 'NN'), ('.', '.')], [('Zoe', 'NP'), ('was', 'BEDZ'), ('interviewing', 'VBG'), ('these', 'DTS'), ('two', 'CD'), ('guys', 'NNS'), (',', ','), ('Jim', 'NP'), ('Logan', 'NP'), ('and', 'CC'), ('Richard', 'NP'), ('Baker', 'NP'), (',', ','), ('who', 'WPS'), ('say', 'VB'), ('that', 'CS'), ('their', 'PP$'), ('company', 'NN'), (',', ','), ('Personal', 'JJ'), ('Audio', 'NN'), (',', ','), ('holds', 'VBZ'), ('the', 'AT'), ('patent', 'NN'), ('on', 'IN'), ('podcasts', 'NN'), ('.', '.')], [('And', 'CC'), ('basically', 'RB'), (',', ','), ('anybody', 'PN'), ('who', 'WPS'), ('makes', 'VBZ'), ('a', 'AT'), ('podcast', 'NN'), ('owes', 'VBZ'), ('them', 'PPO'), ('money', 'NN'), ('.', '.')], [('Zoe', 'NP'), ('pointed', 'VBD'), ('out', 'RP'), ('to', 'TO'), ('them', 'PPO'), ('that', 'CS'), ('their', 'PP$'), ('interview', 'NN'), ('was', 'BEDZ'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('used', 'VBN'), ('on', 'IN'), ('a', 'AT'), ('podcast', 'NN'), ('that', 'CS'), ('has', 'HVZ'), ('not', '*'), ('paid', 'VBN'), ('them', 'PPO'), ('.', '.')], [('So', 'RB'), ('are', 'BER'), ('we', 'PPSS'), ('doing', 'VBG'), ('something', 'PN'), ('illegal', 'JJ'), ('right', 'NN'), ('now', 'RB'), (',', ','), ('though', 'CS'), ('?', '.')], [('I', 'PPSS'), ('would', 'MD'), (\"n't\", 'NN'), ('comment', 'NN'), ('on', 'IN'), ('that', 'DT'), ('.', '.')]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    t3 = nltk.TrigramTagger(train_sents, backoff=t2)\n",
    "    return t3\n",
    "\n",
    "train_sents, test_sents = create_data_sets(tal_sents)\n",
    "ngram_tagger = build_backoff_tagger(brown_tagged)\n",
    "tal_tagged = []\n",
    "for sent in tal_sents:\n",
    "    tal_tagged.append(ngram_tagger.tag(sent)) \n",
    "tal_tagged[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunker:** Code for the proper noun chunker goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Technical Term T = (A | N)+ (N | C) | N\n",
    "grammar = r\"\"\"\n",
    "  T: {(<JJ.|NP.*>)+(<NP.*|CD>)|<NP.*>}   # chunk technical term\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the Chunker:** Test your proper noun recognizer on a lot of sentences to see how well it is working.  You might want to add prepositions in order to improve your results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(T Brian/NP Reed/NP)', '(T William/NP)', '(T Miami/NP)', '(T Dave/NP Mitchell/NP)', '(T Jorge/NP)', '(T Oscar/NP)', '(T David/NP)', '(T Dave/NP)', '(T Miami/NP-TL)', '(T Patricia/NP)', '(T Lisa/NP)', '(T Julie/NP Snyder/NP)', '(T Chicago/NP)', '(T Ben/NP Calhoun/NP)', '(T Colombian/NP)', '(T Miguel/NP)', '(T Colombia/NP)', '(T California/NP)', '(T Lemon/NP)', '(T Florida/NP)', '(T July/NP 15/CD)', '(T Mac/NP)', '(T Chris/NP)', '(T American/NP)', '(T China/NP)', '(T Virginia/NP)', '(T Columbia/NP)', '(T Mike/NP)', '(T California/NP-TL)', '(T Alex/NP Blumberg/NP)', '(T Nancy/NP)', '(T Jersey/NP)', '(T Coke/NP)', '(T Karen/NP Lowe/NP)', '(T Cubans/NPS)', '(T Congress/NP)', '(T Macintosh/NP)', '(T England/NP)', '(T Sarah/NP Koenig/NP)', '(T Rob/NP Geddes/NP)', '(T Jonathan/NP)', '(T Bell/NP-TL)', '(T Matt/NP)', '(T Mario/NP)', '(T God/NP)', '(T Castillo/NP)', '(T Ira/NP)', '(T Anton/NP)', '(T David/NP Mitchell/NP)', '(T Columbia/NP-TL)', '(T Julian/NP)', '(T Pablo/NP)'}\n"
     ]
    }
   ],
   "source": [
    "nps = []\n",
    "for sent in tal_tagged[1000:1600]:\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'T': nps.append(str(subtree))\n",
    "            \n",
    "print(set(nps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FreqDist Results:** After you have your proper noun recognizer working to your satisfaction, below  run it over your entire collection, feed the results into a FreqDist, and then print out the top 20 proper nouns by frequency.  That code goes here, along with the output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(T Jorge/NP)', 84), ('(T Anthony/NP)', 80), ('(T Jon/NP)', 78), ('(T Chris/NP Crawford/NP)', 77), ('(T Sam/NP)', 63), ('(T Joe/NP)', 50), ('(T Adrian/NP)', 42), ('(T Steve/NP)', 35), ('(T American/NP)', 34), ('(T Miguel/NP)', 34)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nps = []\n",
    "for sent in tal_tagged:\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'T': nps.append(str(subtree))\n",
    "\n",
    "fd = nltk.FreqDist(nps)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
