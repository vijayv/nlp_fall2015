{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Scikit-Learn #\n",
    "\n",
    "By John Semerdjian, assisted by Marti Hearst\n",
    "\n",
    "October 2015\n",
    "\n",
    "This is a tutorial on how to use Sciki-Learn in combination with Pandas to train and test a text classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a csv file\n",
    "\n",
    "Let's import data straight into a `DataFrame` from most file types, e.g. `.csv` or `.json`.\n",
    "\n",
    "Download the consumer complaints dataset as a `.csv` file here: https://data.consumerfinance.gov/dataset/Consumer-Complaints/s6ew-h6mp\n",
    "\n",
    "Description from [Consumer Financial Protection Bureau](http://www.consumerfinance.gov/complaintdatabase/):\n",
    "\n",
    "> Each week we send thousands of consumers' complaints about financial products and services to companies for response. Complaints are listed in the database after the company responds or after they’ve had the complaint for 15 calendar days, whichever comes first.\n",
    "\n",
    "> We publish the consumer’s description of what happened if the consumer opts to share it and after taking steps to remove personal information. See our Scrubbing Standard for more details\n",
    "\n",
    "> We don’t verify all the facts alleged in these complaints, but we take steps to confirm a commercial relationship. We may remove complaints if they don’t meet all of the publication criteria. Data is refreshed nightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Consumer_Complaints.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data is really wide! Let's extract a few columns to review\n",
    "\n",
    "We can pass a list of column names to our filter our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [\"Product\", \"Sub-product\", \"Issue\", \"Sub-issue\", \n",
    "        \"Consumer complaint narrative\", \"Company public response\", \n",
    "        \"Company\", \"Company response to consumer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the list of column names in brackets after the name of the DataFrame to subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return all rows that do not have `NaN` in the `Consumer complaint narrative` column\n",
    "\n",
    "The `df[\"Consumer complaint narrative\"].notnull()` argument returns a boolean of values, `True` if the data are not null (`NaN`) and `False` for the rest. We place the array of boolean values within the DataFrame `df` to subset it further.  Farther down we will see how the `reset_index()` function gives us a clean index so we don't have to use the same indices as the larger DataFrame for subsetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_data = df[\"Consumer complaint narrative\"].notnull()\n",
    "filtered_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_narrative = df[filtered_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice where the index starts on the left-most column -- it is no longer in descending order from 0 to the length of the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_narrative[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting with Pandas\n",
    "\n",
    "We can plot the distribution of categories in the `Products` column by chaining the `.value_counts()` and `.plot()` methods after selecting the `Products` column\n",
    "\n",
    "We can then count for each unique value in that column the number of observations within in the DataFrame, which we sort ascending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_product_counts = df_narrative.Product.value_counts(ascending=True)\n",
    "sorted_product_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can plot a horizatontal (`barh`) bar graph to view the results, fix the fiture size to 8x6, and give it a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_product_counts.plot(kind='barh', figsize=(8,6), title=\"Product Categories\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training, development, and test sets\n",
    "\n",
    "First, let's shuffle the rows in our `DataFrame`. There are many ways of splitting our data into training, development, and test sets. We'll use the `numpy` function `random.permutation` to generate a randomized array of row indices. \n",
    "\n",
    "(Alternatively, we can use the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) function from `sklearn.cross_validation` to easily create training and \"test\" sets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_narrative.index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_index = np.random.permutation(df_narrative.index)\n",
    "random_index[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we apply this randomized index, we'll need to reset the index of our new `DataFrame`. This allows us to us the normal indexing approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_narrative.ix[random_index, ['Product', 'Consumer complaint narrative']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `drop=True` option in `reset_index()` resets our rows without adding a new column indicated the old index while `inplace=True` performs the operation in place instead of returning a copy of the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_narrative_shuffled = df_narrative.ix[random_index, ['Product', 'Consumer complaint narrative']]\n",
    "df_narrative_shuffled.reset_index(drop=True, inplace=True)\n",
    "df_narrative_shuffled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 60/20/20 split for training/dev/test sets\n",
    "\n",
    "The `.shape` function returns a tuple of the number of rows and columns in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows, columns = df_narrative_shuffled.shape\n",
    "print(\"Rows:\", rows)\n",
    "print(\"Columns:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = round(rows*.6)\n",
    "dev_size   = round(rows*.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 60% of rows are the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_narrative_shuffled.loc[:train_size]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Followed by the next 20% of rows for the development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dev = df_narrative_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "df_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last 20% are the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = df_narrative_shuffled.loc[dev_size+train_size:].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "After we've wrangled/cleaned/separated our data with `Pandas`, we can start building machine learning algorithms using `Scikit-Learn`, which gives us a rich, unified API to quickly create classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building features from scratch\n",
    "\n",
    "Let's say you have an intuition for the terms you think would be helpful for classification consumer complaints. We can quickly create a column vector for each feature you think of then use a simple classification algorithm for prediction.\n",
    "\n",
    "For now let's just build features to classify credit card-related compliants.\n",
    "\n",
    "I thought of the following features:\n",
    "\n",
    "* character: \"$\"\n",
    "* word: \"payment\"\n",
    "* bigram: \"credit card\"\n",
    "\n",
    "There are two feature processing functions below.  One handles features consisting of one word, and the other handles features consisting of two words.  They count how often the passed in term occurs in the document.  In the bigram case, a FreqDist is needed to keep track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigram_feature(x, unigram):\n",
    "    word_list = x.lower().split(\" \")\n",
    "    return word_list.count(unigram)\n",
    "\n",
    "def bigram_feature(x, bigram):\n",
    "    bigram_tuple = tuple(bigram.split())\n",
    "    word_list = x.lower().split(\" \")\n",
    "    bi = nltk.FreqDist(nltk.bigrams(word_list))\n",
    "    return bi[bigram_tuple]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the dollar sign feature.  It doesn't occur in the first 10 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dollarsign_feature = df_train['Consumer complaint narrative'].apply(lambda x: unigram_feature(x, ('$')))\n",
    "train_dollarsign_feature[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the 'payment' feature.  It occurs twice in document 1 and one time in document 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_payment_feature = df_train['Consumer complaint narrative'].apply(lambda x: unigram_feature(x, ('payment')))\n",
    "train_payment_feature[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_creditcard_feature = df_train['Consumer complaint narrative'].apply(lambda x: bigram_feature(x, ('credit card')))\n",
    "train_creditcard_feature[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring your feature vectors together into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_features = pd.DataFrame({'dollar': train_dollarsign_feature, \n",
    "                                  'payment': train_payment_feature, \n",
    "                                  'creditcard': train_creditcard_feature})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the feature vectors for the development set too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_dollarsign_feature = df_dev['Consumer complaint narrative'].apply(lambda x: unigram_feature(x, ('$')))\n",
    "dev_payment_feature = df_dev['Consumer complaint narrative'].apply(lambda x: unigram_feature(x, ('payment')))\n",
    "dev_creditcard_feature = df_dev['Consumer complaint narrative'].apply(lambda x: bigram_feature(x, ('credit card')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dev_features = pd.DataFrame({'dollar': dev_dollarsign_feature, \n",
    "                                'payment': dev_payment_feature, \n",
    "                                'creditcard': dev_creditcard_feature})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dev_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model using your features\n",
    "\n",
    "We'll build a single Naive Bayes to predict the product category based on the consumer complain features we just created.  The 'fit' function does the training.  We pass in the features and the correct classes that we want as output (the Product column) as the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb_model = nb.fit(df_train_features, df_train.Product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function does the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_predictions = nb_model.predict(df_dev_features)\n",
    "nb_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a `Scikit-Learn` function to calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(df_dev.Product, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch! That's not a very good overall score. Let's look at individual class accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report\n",
    "\n",
    "Another way of evaluating the performance of your models is to use `Scikit-Learn`'s `classification_report` function.\n",
    "\n",
    "* **Precision**:\n",
    "$$\\frac{TP} {TP+FP}$$\n",
    "\n",
    "* **Recall, Sensitivity, TP Rate**:\n",
    "$$\\frac{TP} {TP+FN}$$\n",
    "\n",
    "* **$F_1$ Measure**:\n",
    "$$F _1 = 2 \\frac{PR} {P + R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class labels are usually returned sorted in alphabetical/numerical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_labels = np.sort(df_train.Product.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the full report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(df_dev.Product, nb_predictions, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature using overall word counts\n",
    "\n",
    "Another strategy of creating features is to use *all* the words in our collection. [`CountVectorizer()`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) accepts an array of text and converts the text through tokenization and counting unique terms, thereby generating our so-called \"bag of words\".\n",
    "\n",
    "In other words, for each row in our `DataFrame` we get a long vector/array of the counts of each word.  You can modify the tokenizer, remove stop words, generate n-gram features, and perform other types of text processing. \n",
    "\n",
    "Here are some options to explore:\n",
    "\n",
    "`token_pattern : string`\n",
    "\n",
    "> Regular expression denoting what constitutes a “token”, only used if tokenize == ‘word’. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "`min_df : float in range [0.0, 1.0] or int, default=1`\n",
    "\n",
    "> When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "`max_features : int or None, default=None`\n",
    "\n",
    "> If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "`stop_words : string {‘english’}, list, or None (default)`\n",
    "\n",
    "> If ‘english’, a built-in stop word list for English is used. If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "\n",
    "`ngram_range : tuple (min_n, max_n)`\n",
    "\n",
    "> The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "\n",
    "\n",
    "If you're interested in using Tf-Idf instead of counts, check out [`TfidfVectorizer()`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "The [feature extraction documentation](http://scikit-learn.org/stable/modules/feature_extraction.html) from Scikit-Learn is also very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use n-gram between 1 to 2 values with a simplified token pattern to find ngrams that occur more than 5 times in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually inspect what the tokenizer does by passing in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = vec.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer(\"What's the warranty for this $40.00 toaster?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, we'll use the default tokenization pattern and all the text across the **training set** to create our feature vectors. The `fit_transform()` performs this activity and returns a sparse array of the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train_feature_sparse = vec.fit_transform(df_train[\"Consumer complaint narrative\"])\n",
    "arr_train_feature_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse matrix is an efficient way of storing data where most values are 0. Just for information's sake, below we convert the sparse array into a normal array to get a better sense of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "arr_train_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of getting back a `Pandas DataFrame`, we get back a `numpy array` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting our features\n",
    "\n",
    "To see what our features are, we'll use the `get_feature_names()` function on our `vec` object that we fitted and transformed.  Remember that we asked for both unigrams and bigrams when we created the feature vector.\n",
    "\n",
    "You'll see a lot of nonsensical features there. What are some strategies that you can think of to make the tokenizer produce more informative features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_labels = vec.get_feature_names()\n",
    "feature_labels[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try matching one word from the first row in our `DataFrame` to it's respective position in our feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row0 = df_train.ix[0, 'Consumer complaint narrative']\n",
    "row0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we search for a word in the feature labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_index = feature_labels.index('credit')\n",
    "feature_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should expect the count of the number of occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train_feature[0, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build our model, we'll need to perform the same transformation on our dev and test sets as we did on the training set.  To do this, we use the `transform()` function.\n",
    "\n",
    "Note that we use 'transform()' and not  `fit_transform()` since that would reset the features using the text from the dev or test set. We only want to use the features that are present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_dev_feature_sparse = vec.transform(df_dev[\"Consumer complaint narrative\"])\n",
    "arr_dev_feature = arr_dev_feature_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have way more features than observations!\n",
    "\n",
    "This is a good time to consider dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common features\n",
    "\n",
    "Let's plot the distribution of counts for the massive feature set we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_sum = arr_train_feature.sum(axis=0)\n",
    "\n",
    "df_feature_sum = pd.DataFrame({'counts': feature_sum})\n",
    "df_feature_sum.index = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_feature_sum.sort('counts', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_feature_sum.sort('counts', ascending=False)[:50].plot(kind='barh', figsize=(7,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many of these features are stopwords\n",
    "\n",
    "How might we fix this?  (Hint: this makes the algorithm work much better on this classification problem.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's manually reduce our dimensions by using only use the top N features in our training set\n",
    "\n",
    "Since so many of our features rarely occur, let's (arbitrary) cap our features to the top 1000 most common n-grams using the `max_features` variable within the `CountVectorizer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=5, max_features=1000)\n",
    "arr_train_feature_sparse = vec.fit_transform(df_train[\"Consumer complaint narrative\"])\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "arr_train_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that if you transform the training set, you also have to transfomr the development set using this new vector vec to get the desired effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_dev_feature_sparse = vec.transform(df_dev[\"Consumer complaint narrative\"])\n",
    "arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "arr_dev_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train two machine learning classification models\n",
    "\n",
    "### Naive Bayes (generative)\n",
    "\n",
    "This initializes a [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) model which we then fit in the next row. We pass the training features along with their true labels, `df_train.Product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb_model = nb.fit(arr_train_feature, df_train.Product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily predict the labels using our new model and the feature vectors from the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_predictions = nb_model.predict(arr_dev_feature)\n",
    "nb_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the accuracy of our Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(df_dev.Product, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at the class labels, let's look at the probability of predicting each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_predictions_probs = nb_model.predict_proba(arr_dev_feature)\n",
    "nb_predictions_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predicted probabilities of each class for the first observation in our dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(nb_predictions_probs[0,:])\n",
    "plt.xticks(np.arange(11), class_labels, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the text look like for the first observation in the dev set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dev.loc[0, 'Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's real label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dev.loc[0, 'Product']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (discriminative)\n",
    "\n",
    "We perform the same steps here as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train.Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_predictions = logreg_model.predict(arr_dev_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(df_dev.Product, logreg_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the probabilities for the first observation in the dev set between logistic regression and naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_predictions_probs = logreg_model.predict_proba(arr_dev_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(logreg_predictions_probs[0,:], label='Logistic Regression')\n",
    "plt.plot(nb_predictions_probs[0,:], label='Naive Bayes')\n",
    "plt.xticks(np.arange(11), class_labels, rotation='vertical')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Confusion Matrix\n",
    "\n",
    "A confusion matrix is handy when inspecting the errors from a multi-class classification problem. Each row and column represents the how well our predicted labels matched their true values. \n",
    "\n",
    "`Scikit-Learn` has a function called `confusion_matrix()` which produces array with this data. However, the `Pandas crosstab` function does a better job displaying this information.\n",
    "\n",
    "Remember, if our model had 100% accuracy, we would expect only the diagonal values to be populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_dev.Product, nb_predictions, \n",
    "            rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_dev.Product, logreg_predictions, \n",
    "            rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a function to help you visualize the confusion matrix data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title, target_names, cmap=plt.cm.coolwarm):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix\n",
    "\n",
    "Confusion Matrix names are sorted by the `confusion_matrix` function in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_cm = confusion_matrix(df_dev.Product, nb_predictions)\n",
    "plot_confusion_matrix(nb_cm, \"Naive Bayes Confusion Matrix\", class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_cm = confusion_matrix(df_dev.Product, logreg_predictions)\n",
    "plot_confusion_matrix(logreg_cm, \"Logistic Regression Confusion Matrix\", class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(df_dev.Product, nb_predictions, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(df_dev.Product, logreg_predictions, target_names=class_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
