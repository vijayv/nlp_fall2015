{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal of this assignment is to give you an opportunity to get hands-on experience with the unsupervised methods we learned about.\n",
    "\n",
    "Part 1:\n",
    "\n",
    "(a) Experiment with either k-means clustering or LDA on your adopted document collection to try to find topics in the collection.   Be sure to try a few different values of k.  (If you want to use some other variant of clustering, that is fine.)\n",
    "\n",
    "(b) Show your output in some easy-to-digest form.\n",
    "\n",
    "(c) Discuss how well it did or did not work.\n",
    "\n",
    "(d) (Optional) Compare to a WordNet grouping algorithm, such as those students came up with in the keyphrase assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode Name</th>\n",
       "      <th>Episode Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1 Party School</td>\n",
       "      <td>('#1 Party School', '&lt;EPISODE NUMBER:396&gt; &lt;EPI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>('2010', '&lt;EPISODE NUMBER:397&gt; &lt;EPISODE NAME:2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Long Shot</td>\n",
       "      <td>('Long Shot', '&lt;EPISODE NUMBER:398&gt; &lt;EPISODE N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents Unknown</td>\n",
       "      <td>('Contents Unknown', '&lt;EPISODE NUMBER:399&gt; &lt;EP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stories Pitched by Our Parents</td>\n",
       "      <td>('Stories Pitched by Our Parents', '&lt;EPISODE N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Episode Name  \\\n",
       "0                 #1 Party School   \n",
       "1                            2010   \n",
       "2                       Long Shot   \n",
       "3                Contents Unknown   \n",
       "4  Stories Pitched by Our Parents   \n",
       "\n",
       "                                  Episode Transcript  \n",
       "0  ('#1 Party School', '<EPISODE NUMBER:396> <EPI...  \n",
       "1  ('2010', '<EPISODE NUMBER:397> <EPISODE NAME:2...  \n",
       "2  ('Long Shot', '<EPISODE NUMBER:398> <EPISODE N...  \n",
       "3  ('Contents Unknown', '<EPISODE NUMBER:399> <EP...  \n",
       "4  ('Stories Pitched by Our Parents', '<EPISODE N...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tal_text = pd.read_csv(\"../tal_stories/tal_text_broad.txt\")\n",
    "tal_text.columns = [\"Episode Name\", \"Episode Transcript\"]\n",
    "tal_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_vocab_stemmed = []\n",
    "total_vocab_tokenized = []\n",
    "for each in tal_text[\"Episode Transcript\"]:\n",
    "    allwords_stemmed = tokenize_and_stem(each)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(each)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1098587 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 858 ms, total: 1min 19s\n",
      "Wall time: 1min 30s\n",
      "(101, 2228)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=.8, max_features=200000,\n",
    "                                    min_df=.2, stop_words='english',\n",
    "                                  use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(tal_text[\"Episode Transcript\"])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 343 ms, sys: 5.4 ms, total: 348 ms\n",
      "Wall time: 719 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster_algo1.pkl', 'cluster_algo1.pkl_01.npy', 'cluster_algo1.pkl_02.npy']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# joblib.dump(km, 'cluster_algo1.pkl')\n",
    "\n",
    "km = joblib.load('cluster_algo1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    33\n",
       "3    23\n",
       "1    19\n",
       "2    16\n",
       "4    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tal_text[\"Cluster\"] = clusters\n",
    "tal_text[\"Cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: b'interviews', b'interviews', b'Paul', b'Mike', b'Adam', b'drug', b'David', b'students', b'game', b'girls', b'Wall', b'parents', b'dad', b'police', b'court', b'JAMES', b'dog', b'cells', b'marked', b'cancer',\n",
      "\n",
      "Cluster 0 Episode Names: Long Shot\n",
      " Contents Unknown\n",
      " Parent Trap\n",
      " Save the Day\n",
      " Enemy Camp 2010\n",
      " True Urban Legends \n",
      " Island Time \n",
      " Held Hostage\n",
      " First Contact\n",
      " Million Dollar Idea\n",
      " Neighborhood Watch\n",
      " Kid Politics\n",
      " Slow To React\n",
      " Tough Room 2011\n",
      " Very Tough Love\n",
      " Know When To Fold 'Em\n",
      " Amusement Park\n",
      " Living Without (2011)\n",
      " So Crazy It Just Might Work\n",
      " Poultry Slam 2011\n",
      " Nemeses\n",
      " Mr. Daisey and the Apple Factory\n",
      " What I Did For Love\n",
      " Retraction\n",
      " Invisible Made Visible\n",
      " Americans in China\n",
      " Hiding in Plain Sight\n",
      " The Convert\n",
      " Back to School\n",
      " This Week\n",
      " Lights, Camera, Christmas!\n",
      " Self-Improvement Kick \n",
      " Valentine's Day\n",
      "\n",
      "\n",
      "Cluster 1 words: b'Alex', b'Alex', b'Blumberg', b'David', b'company', b'Adam', b'governments', b'banks', b'economy', b'millions', b'Economic', b'crisis', b'lawyer', b'billion', b'created', b'frankly', b'dollars', b'buying', b'fed', b'governor',\n",
      "\n",
      "Cluster 1 Episode Names: 2010\n",
      " NUMMI\n",
      " Inside Job\n",
      " Social Contract\n",
      " Crybabies\n",
      " Toxie\n",
      " The Invention of Money\n",
      " How To Create a Job\n",
      " When Patents Attack!\n",
      " Adventure!\n",
      " Continental Breakup\n",
      " Take the Money and Run for Office\n",
      " Mortal Vs. Venial\n",
      " Our Friend David\n",
      " Loopholes\n",
      " Getting Away With It\n",
      " Trends With Benefits\n",
      " Tribes\n",
      " When Patents Attack... Part Two!\n",
      "\n",
      "\n",
      "Cluster 2 words: b'Sarah', b'Sarah', b'Koenig', b'students', b'dad', b'Steve', b'interviews', b'interviews', b'Lisa', b'Pollak', b'Lisa', b'tax', b'MALE', b'football', b'town', b'drink', b'Jane', b'Jane', b'Matt', b'father',\n",
      "\n",
      "Cluster 2 Episode Names: #1 Party School\n",
      " Stories Pitched by Our Parents\n",
      " Georgia Rambler\n",
      " Petty Tyrant\n",
      " Last Man Standing\n",
      " Game Changer\n",
      " Gossip\n",
      " Middle School\n",
      " Back to Penn State\n",
      " What Kind of Country\n",
      " Blackjack\n",
      " Switcheroo\n",
      " Red State Blue State\n",
      " Surrogates\n",
      " No Coincidence, No Story!\n",
      " Dr. Gilmer and Mr. Hyde\n",
      "\n",
      "\n",
      "Cluster 3 words: b'Ben', b'John', b'Calhoun', b'Ben', b'mom', b'interviews', b'interviews', b'Republicans', b'dad', b'police', b'David', b'hospital', b'I\\\\', b\"'m\", b'Parties', b'doctor', b'gun', b'Christmas', b'students', b'Jonathan',\n",
      "\n",
      "Cluster 3 Episode Names: Right to Remain Silent\n",
      " This Party Sucks\n",
      " Comedians of Christmas Comedy Special\n",
      " Original Recipe\n",
      " See No Evil\n",
      " This Week\n",
      " The Psychopath Test\n",
      " Old Boys Network\n",
      " Father's Day 2011\n",
      " A House Divided\n",
      " Ten Years In\n",
      " The Incredible Case of the P.I. Moms\n",
      " Reap What You Sow\n",
      " Play the Part\n",
      " Own Worst Enemy\n",
      " Show Me The Way\n",
      " What Doesn't Kill You\n",
      " Little War on the Prairie\n",
      " Doppelg?ngers\n",
      " Harper High School, Part One\n",
      " Harper High School, Part Two\n",
      " Hit the Road\n",
      " Hot In My Backyard\n",
      "\n",
      "\n",
      "Cluster 4 words: b'Nancy', b'Nancy', b'Updike', b'translator', b'soldiers', b'Army', b'dog', b'SPANISH', b'military', b'Sarah', b'boys', b'war', b'arrested', b'father', b'son', b'Brian', b'Reed', b'Brian', b'drug', b'interpreting',\n",
      "\n",
      "Cluster 4 Episode Names: The Bridge\n",
      " Iraq After Us\n",
      " Oh You Shouldn't Have\n",
      " Will They Know Me Back Home?\n",
      " Fine Print 2011\n",
      " Thugs\n",
      " What Happened At Dos Erres\n",
      " Send a Message\n",
      " Animal Sacrifice\n",
      " Picture Show\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(\" %s\" % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=\",\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(\"Cluster %d Episode Names:\" %i, end='')\n",
    "    for title in tal_text[tal_text[\"Cluster\"] == i]['Episode Name']:\n",
    "        print(' %s' % title, end='\\n')\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "After the first run of the clustering algorithm, it is clear that most of the clusters seem to be be affected by the proper nouns, or names of individuals.\n",
    "\n",
    "```\n",
    "Top terms per cluster:\n",
    "\n",
    "Cluster 0 words: b'interviews', b'interviews', b'Paul', b'Mike', b'Adam', b'drug', b'David', b'students', b'game', b'girls', b'Wall', b'parents', b'dad', b'police', b'court', b'JAMES', b'dog', b'cells', b'marked', b'cancer',\n",
    "\n",
    "Cluster 0 Episode Names: Long Shot\n",
    " Contents Unknown\n",
    " Parent Trap\n",
    " Save the Day\n",
    " Enemy Camp 2010\n",
    " True Urban Legends \n",
    " Island Time \n",
    " Held Hostage\n",
    " First Contact\n",
    " Million Dollar Idea\n",
    " Neighborhood Watch\n",
    " Kid Politics\n",
    " Slow To React\n",
    " Tough Room 2011\n",
    " Very Tough Love\n",
    " Know When To Fold 'Em\n",
    " Amusement Park\n",
    " Living Without (2011)\n",
    " So Crazy It Just Might Work\n",
    " Poultry Slam 2011\n",
    " Nemeses\n",
    " Mr. Daisey and the Apple Factory\n",
    " What I Did For Love\n",
    " Retraction\n",
    " Invisible Made Visible\n",
    " Americans in China\n",
    " Hiding in Plain Sight\n",
    " The Convert\n",
    " Back to School\n",
    " This Week\n",
    " Lights, Camera, Christmas!\n",
    " Self-Improvement Kick \n",
    " Valentine's Day\n",
    "\n",
    "\n",
    "Cluster 1 words: b'Alex', b'Alex', b'Blumberg', b'David', b'company', b'Adam', b'governments', b'banks', b'economy', b'millions', b'Economic', b'crisis', b'lawyer', b'billion', b'created', b'frankly', b'dollars', b'buying', b'fed', b'governor',\n",
    "\n",
    "Cluster 1 Episode Names: 2010\n",
    " NUMMI\n",
    " Inside Job\n",
    " Social Contract\n",
    " Crybabies\n",
    " Toxie\n",
    " The Invention of Money\n",
    " How To Create a Job\n",
    " When Patents Attack!\n",
    " Adventure!\n",
    " Continental Breakup\n",
    " Take the Money and Run for Office\n",
    " Mortal Vs. Venial\n",
    " Our Friend David\n",
    " Loopholes\n",
    " Getting Away With It\n",
    " Trends With Benefits\n",
    " Tribes\n",
    " When Patents Attack... Part Two!\n",
    "\n",
    "\n",
    "Cluster 2 words: b'Sarah', b'Sarah', b'Koenig', b'students', b'dad', b'Steve', b'interviews', b'interviews', b'Lisa', b'Pollak', b'Lisa', b'tax', b'MALE', b'football', b'town', b'drink', b'Jane', b'Jane', b'Matt', b'father',\n",
    "\n",
    "Cluster 2 Episode Names: #1 Party School\n",
    " Stories Pitched by Our Parents\n",
    " Georgia Rambler\n",
    " Petty Tyrant\n",
    " Last Man Standing\n",
    " Game Changer\n",
    " Gossip\n",
    " Middle School\n",
    " Back to Penn State\n",
    " What Kind of Country\n",
    " Blackjack\n",
    " Switcheroo\n",
    " Red State Blue State\n",
    " Surrogates\n",
    " No Coincidence, No Story!\n",
    " Dr. Gilmer and Mr. Hyde\n",
    "\n",
    "\n",
    "Cluster 3 words: b'Ben', b'John', b'Calhoun', b'Ben', b'mom', b'interviews', b'interviews', b'Republicans', b'dad', b'police', b'David', b'hospital', b'I\\\\', b\"'m\", b'Parties', b'doctor', b'gun', b'Christmas', b'students', b'Jonathan',\n",
    "\n",
    "Cluster 3 Episode Names: Right to Remain Silent\n",
    " This Party Sucks\n",
    " Comedians of Christmas Comedy Special\n",
    " Original Recipe\n",
    " See No Evil\n",
    " This Week\n",
    " The Psychopath Test\n",
    " Old Boys Network\n",
    " Father's Day 2011\n",
    " A House Divided\n",
    " Ten Years In\n",
    " The Incredible Case of the P.I. Moms\n",
    " Reap What You Sow\n",
    " Play the Part\n",
    " Own Worst Enemy\n",
    " Show Me The Way\n",
    " What Doesn't Kill You\n",
    " Little War on the Prairie\n",
    " Doppelg?ngers\n",
    " Harper High School, Part One\n",
    " Harper High School, Part Two\n",
    " Hit the Road\n",
    " Hot In My Backyard\n",
    "\n",
    "\n",
    "Cluster 4 words: b'Nancy', b'Nancy', b'Updike', b'translator', b'soldiers', b'Army', b'dog', b'SPANISH', b'military', b'Sarah', b'boys', b'war', b'arrested', b'father', b'son', b'Brian', b'Reed', b'Brian', b'drug', b'interpreting',\n",
    "\n",
    "Cluster 4 Episode Names: The Bridge\n",
    " Iraq After Us\n",
    " Oh You Shouldn't Have\n",
    " Will They Know Me Back Home?\n",
    " Fine Print 2011\n",
    " Thugs\n",
    " What Happened At Dos Erres\n",
    " Send a Message\n",
    " Animal Sacrifice\n",
    " Picture Show\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "\n",
    "Experiment with Word2Vec to find related terms for terms in your collection.  I recommend using the large pre-trained collection that is in the notebook we discussed in class.  \n",
    "\n",
    "(a) Select five nouns of interest from your collection, and compare what WordNet finds as the first 3 synsets to what Word2Vec finds as the top 5 rated similar nouns (using the most_similar() function).  State results are better for your collection in each case?  (you may use negative evidence if you like, by providing positive and negative example words).\n",
    "\n",
    "(b) Do the same for 5 adjectives.\n",
    "\n",
    "(c) Do the same for 5 verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
