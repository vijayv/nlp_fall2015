{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal of this assignment is to give you an opportunity to get hands-on experience with the unsupervised methods we learned about.\n",
    "\n",
    "Part 1:\n",
    "\n",
    "(a) Experiment with either k-means clustering or LDA on your adopted document collection to try to find topics in the collection.   Be sure to try a few different values of k.  (If you want to use some other variant of clustering, that is fine.)\n",
    "\n",
    "(b) Show your output in some easy-to-digest form.\n",
    "\n",
    "(c) Discuss how well it did or did not work.\n",
    "\n",
    "(d) (Optional) Compare to a WordNet grouping algorithm, such as those students came up with in the keyphrase assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens = list(filter(lambda x: not x[0].isupper(), tokens))\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:        \n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens = list(filter(lambda x: not x[0].isupper(), tokens))\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode Name</th>\n",
       "      <th>Episode Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1 Party School</td>\n",
       "      <td>('#1 Party School', '&lt;EPISODE NUMBER:396&gt; &lt;EPI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>('2010', '&lt;EPISODE NUMBER:397&gt; &lt;EPISODE NAME:2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Long Shot</td>\n",
       "      <td>('Long Shot', '&lt;EPISODE NUMBER:398&gt; &lt;EPISODE N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents Unknown</td>\n",
       "      <td>('Contents Unknown', '&lt;EPISODE NUMBER:399&gt; &lt;EP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stories Pitched by Our Parents</td>\n",
       "      <td>('Stories Pitched by Our Parents', '&lt;EPISODE N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Episode Name  \\\n",
       "0                 #1 Party School   \n",
       "1                            2010   \n",
       "2                       Long Shot   \n",
       "3                Contents Unknown   \n",
       "4  Stories Pitched by Our Parents   \n",
       "\n",
       "                                  Episode Transcript  \n",
       "0  ('#1 Party School', '<EPISODE NUMBER:396> <EPI...  \n",
       "1  ('2010', '<EPISODE NUMBER:397> <EPISODE NAME:2...  \n",
       "2  ('Long Shot', '<EPISODE NUMBER:398> <EPISODE N...  \n",
       "3  ('Contents Unknown', '<EPISODE NUMBER:399> <EP...  \n",
       "4  ('Stories Pitched by Our Parents', '<EPISODE N...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tal_text = pd.read_csv(\"../tal_stories/tal_text_broad.txt\")\n",
    "tal_text.columns = [\"Episode Name\", \"Episode Transcript\"]\n",
    "tal_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for each in tal_text[\"Episode Transcript\"]:\n",
    "    allwords_stemmed = tokenize_and_stem(each)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(each)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)\n",
    "vocab_frame = vocab_frame.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 670 ms, total: 1min 18s\n",
      "Wall time: 1min 21s\n",
      "(101, 2228)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=.8, max_features=200000,\n",
    "                                    min_df=.2, stop_words='english',\n",
    "                                  use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(tal_text[\"Episode Transcript\"])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 343 ms, sys: 5.04 ms, total: 348 ms\n",
      "Wall time: 457 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster_algo2.pkl', 'cluster_algo2.pkl_01.npy', 'cluster_algo2.pkl_02.npy']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(km, 'cluster_algo2.pkl')\n",
    "\n",
    "# km = joblib.load('cluster_algo2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    46\n",
       "2    16\n",
       "0    16\n",
       "4    12\n",
       "1    11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tal_text[\"Cluster\"] = clusters\n",
    "tal_text[\"Cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: nan, nan, nan, students, nan, dad, male, taxing, drinking, football, town, interview, interview, nan, quote, father, girls, middle, nan, card,\n",
      "\n",
      "Cluster 0 Episode Names: #1 Party School\n",
      " Georgia Rambler\n",
      " Petty Tyrant\n",
      " Last Man Standing\n",
      " Will They Know Me Back Home?\n",
      " Game Changer\n",
      " Gossip\n",
      " Middle School\n",
      " Back to Penn State\n",
      " What Kind of Country\n",
      " Blackjack\n",
      " Switcheroo\n",
      " Red State Blue State\n",
      " Surrogates\n",
      " No Coincidence, No Story!\n",
      " Dr. Gilmer and Mr. Hyde\n",
      "\n",
      "\n",
      "Cluster 1 words: nan, letter, mother, hill, nan, mom, wall, parents, heart, adamantly, doctors, interview, interview, churches, mark, bed, inside, subject, cancer, box,\n",
      "\n",
      "Cluster 1 Episode Names: Contents Unknown\n",
      " Parent Trap\n",
      " Enemy Camp 2010\n",
      " Toxie\n",
      " Comedians of Christmas Comedy Special\n",
      " Slow To React\n",
      " Living Without (2011)\n",
      " Invisible Made Visible\n",
      " Show Me The Way\n",
      " Our Friend David\n",
      " Tribes\n",
      "\n",
      "\n",
      "Cluster 2 words: nan, nan, nan, company, adamantly, government, nan, economic, banks, million, dollar, economy, frank, crisis, create, invest, billion, fed, governor, petering,\n",
      "\n",
      "Cluster 2 Episode Names: 2010\n",
      " NUMMI\n",
      " Inside Job\n",
      " Social Contract\n",
      " Million Dollar Idea\n",
      " Crybabies\n",
      " The Invention of Money\n",
      " How To Create a Job\n",
      " When Patents Attack!\n",
      " Adventure!\n",
      " Continental Breakup\n",
      " Take the Money and Run for Office\n",
      " Mortal Vs. Venial\n",
      " Loopholes\n",
      " Getting Away With It\n",
      " When Patents Attack... Part Two!\n",
      "\n",
      "\n",
      "Cluster 3 words: interview, interview, nan, dad, nan, nan, translated, nan, dogs, mike, drugs, police, mom, girls, tests, nan, students, nan, soldiers, government,\n",
      "\n",
      "Cluster 3 Episode Names: Long Shot\n",
      " Stories Pitched by Our Parents\n",
      " Save the Day\n",
      " True Urban Legends \n",
      " The Bridge\n",
      " Held Hostage\n",
      " First Contact\n",
      " Iraq After Us\n",
      " Neighborhood Watch\n",
      " Kid Politics\n",
      " Tough Room 2011\n",
      " Oh You Shouldn't Have\n",
      " Very Tough Love\n",
      " Know When To Fold 'Em\n",
      " Fine Print 2011\n",
      " This Week\n",
      " The Psychopath Test\n",
      " Old Boys Network\n",
      " Father's Day 2011\n",
      " Thugs\n",
      " Amusement Park\n",
      " Ten Years In\n",
      " The Incredible Case of the P.I. Moms\n",
      " So Crazy It Just Might Work\n",
      " Poultry Slam 2011\n",
      " Nemeses\n",
      " Mr. Daisey and the Apple Factory\n",
      " Reap What You Sow\n",
      " What I Did For Love\n",
      " Play the Part\n",
      " Retraction\n",
      " What Happened At Dos Erres\n",
      " Americans in China\n",
      " Hiding in Plain Sight\n",
      " The Convert\n",
      " Back to School\n",
      " Send a Message\n",
      " What Doesn't Kill You\n",
      " Animal Sacrifice\n",
      " This Week\n",
      " Lights, Camera, Christmas!\n",
      " Self-Improvement Kick \n",
      " Valentine's Day\n",
      " Trends With Benefits\n",
      " Picture Show\n",
      " Hit the Road\n",
      "\n",
      "\n",
      "Cluster 4 words: nan, nan, nan, nan, republican, adamantly, police, interview, interview, students, gun, nan, shots, nan, rich, shooting, war, brothers, hospital, tea,\n",
      "\n",
      "Cluster 4 Episode Names: Island Time \n",
      " Right to Remain Silent\n",
      " This Party Sucks\n",
      " Original Recipe\n",
      " See No Evil\n",
      " A House Divided\n",
      " Own Worst Enemy\n",
      " Little War on the Prairie\n",
      " Doppelg?ngers\n",
      " Harper High School, Part One\n",
      " Harper High School, Part Two\n",
      " Hot In My Backyard\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(\" %s\" % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0], end=\",\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(\"Cluster %d Episode Names:\" %i, end='')\n",
    "    for title in tal_text[tal_text[\"Cluster\"] == i]['Episode Name']:\n",
    "        print(' %s' % title, end='\\n')\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "After the first run of the clustering algorithm, it was clear that most of the clusters seem to be be affected by the proper nouns, or names of individuals. You can see the output of the first run here:\n",
    "\n",
    "```\n",
    "Top terms per cluster:\n",
    "\n",
    "Cluster 0 words: b'interviews', b'interviews', b'Paul', b'Mike', b'Adam', b'drug', b'David', b'students', b'game', b'girls', b'Wall', b'parents', b'dad', b'police', b'court', b'JAMES', b'dog', b'cells', b'marked', b'cancer',\n",
    "\n",
    "Cluster 0 Episode Names: Long Shot\n",
    " Contents Unknown\n",
    " Parent Trap\n",
    " Save the Day\n",
    " Enemy Camp 2010\n",
    " True Urban Legends \n",
    " Island Time \n",
    " Held Hostage\n",
    " First Contact\n",
    " Million Dollar Idea\n",
    " Neighborhood Watch\n",
    " Kid Politics\n",
    " Slow To React\n",
    " Tough Room 2011\n",
    " Very Tough Love\n",
    " Know When To Fold 'Em\n",
    " Amusement Park\n",
    " Living Without (2011)\n",
    " So Crazy It Just Might Work\n",
    " Poultry Slam 2011\n",
    " Nemeses\n",
    " Mr. Daisey and the Apple Factory\n",
    " What I Did For Love\n",
    " Retraction\n",
    " Invisible Made Visible\n",
    " Americans in China\n",
    " Hiding in Plain Sight\n",
    " The Convert\n",
    " Back to School\n",
    " This Week\n",
    " Lights, Camera, Christmas!\n",
    " Self-Improvement Kick \n",
    " Valentine's Day\n",
    "\n",
    "\n",
    "Cluster 1 words: b'Alex', b'Alex', b'Blumberg', b'David', b'company', b'Adam', b'governments', b'banks', b'economy', b'millions', b'Economic', b'crisis', b'lawyer', b'billion', b'created', b'frankly', b'dollars', b'buying', b'fed', b'governor',\n",
    "\n",
    "Cluster 1 Episode Names: 2010\n",
    " NUMMI\n",
    " Inside Job\n",
    " Social Contract\n",
    " Crybabies\n",
    " Toxie\n",
    " The Invention of Money\n",
    " How To Create a Job\n",
    " When Patents Attack!\n",
    " Adventure!\n",
    " Continental Breakup\n",
    " Take the Money and Run for Office\n",
    " Mortal Vs. Venial\n",
    " Our Friend David\n",
    " Loopholes\n",
    " Getting Away With It\n",
    " Trends With Benefits\n",
    " Tribes\n",
    " When Patents Attack... Part Two!\n",
    "\n",
    "\n",
    "Cluster 2 words: b'Sarah', b'Sarah', b'Koenig', b'students', b'dad', b'Steve', b'interviews', b'interviews', b'Lisa', b'Pollak', b'Lisa', b'tax', b'MALE', b'football', b'town', b'drink', b'Jane', b'Jane', b'Matt', b'father',\n",
    "\n",
    "Cluster 2 Episode Names: #1 Party School\n",
    " Stories Pitched by Our Parents\n",
    " Georgia Rambler\n",
    " Petty Tyrant\n",
    " Last Man Standing\n",
    " Game Changer\n",
    " Gossip\n",
    " Middle School\n",
    " Back to Penn State\n",
    " What Kind of Country\n",
    " Blackjack\n",
    " Switcheroo\n",
    " Red State Blue State\n",
    " Surrogates\n",
    " No Coincidence, No Story!\n",
    " Dr. Gilmer and Mr. Hyde\n",
    "\n",
    "\n",
    "Cluster 3 words: b'Ben', b'John', b'Calhoun', b'Ben', b'mom', b'interviews', b'interviews', b'Republicans', b'dad', b'police', b'David', b'hospital', b'I\\\\', b\"'m\", b'Parties', b'doctor', b'gun', b'Christmas', b'students', b'Jonathan',\n",
    "\n",
    "Cluster 3 Episode Names: Right to Remain Silent\n",
    " This Party Sucks\n",
    " Comedians of Christmas Comedy Special\n",
    " Original Recipe\n",
    " See No Evil\n",
    " This Week\n",
    " The Psychopath Test\n",
    " Old Boys Network\n",
    " Father's Day 2011\n",
    " A House Divided\n",
    " Ten Years In\n",
    " The Incredible Case of the P.I. Moms\n",
    " Reap What You Sow\n",
    " Play the Part\n",
    " Own Worst Enemy\n",
    " Show Me The Way\n",
    " What Doesn't Kill You\n",
    " Little War on the Prairie\n",
    " Doppelg?ngers\n",
    " Harper High School, Part One\n",
    " Harper High School, Part Two\n",
    " Hit the Road\n",
    " Hot In My Backyard\n",
    "\n",
    "\n",
    "Cluster 4 words: b'Nancy', b'Nancy', b'Updike', b'translator', b'soldiers', b'Army', b'dog', b'SPANISH', b'military', b'Sarah', b'boys', b'war', b'arrested', b'father', b'son', b'Brian', b'Reed', b'Brian', b'drug', b'interpreting',\n",
    "\n",
    "Cluster 4 Episode Names: The Bridge\n",
    " Iraq After Us\n",
    " Oh You Shouldn't Have\n",
    " Will They Know Me Back Home?\n",
    " Fine Print 2011\n",
    " Thugs\n",
    " What Happened At Dos Erres\n",
    " Send a Message\n",
    " Animal Sacrifice\n",
    " Picture Show\n",
    "```\n",
    "\n",
    "Therefore, I modified the tokenizer and the stemmer to pos_tag each word and I planned on removing any proper nouns. However, the pos_tagger significantly slowed the process and did not complete tokenizing my corpus after 15 minutes. As a substitute, instead of tagging the words, I decided to just remove words that started with a capital letter. This worked much faster, however, I may have wrongfully excluded some words.\n",
    "\n",
    "Overall, the clustering seems to have worked quite well using the tf-idf vectorizer. It was interesting to see the various episodes grouped together that I recognize as being similar in topics. For example, cluster 2 had both the patent episodes as well as the episode on the NUMMI car plant. Given the other topics, I thought this would proper classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "\n",
    "Experiment with Word2Vec to find related terms for terms in your collection.  I recommend using the large pre-trained collection that is in the notebook we discussed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.data import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_model = gensim.models.Word2Vec(brown.sents())\n",
    "\n",
    "# It might take some time to train the model. So, after it is trained, it can be saved as follows:\n",
    "\n",
    "brown_model.save('brown.embedding')\n",
    "new_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Select five nouns of interest from your collection, and compare what WordNet finds as the first 3 synsets to what Word2Vec finds as the top 5 rated similar nouns (using the most_similar() function).  State results are better for your collection in each case?  (you may use negative evidence if you like, by providing positive and negative example words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students\n",
      "Word2Vec Results\n",
      "[('enroll', 0.7520958781242371), ('automobiles', 0.7386108040809631), ('matters', 0.7279121279716492), ('countries', 0.7262592315673828), ('counterparts', 0.7189865708351135)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('student.n.01'), Synset('scholar.n.01')]\n",
      "\n",
      "****************************************************************************************************\n",
      "economy\n",
      "Word2Vec Results\n",
      "[('outlook', 0.8700981736183167), ('union', 0.8654215335845947), ('acceptance', 0.8580105304718018), ('leadership', 0.853107213973999), ('commitment', 0.8510946035385132)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('economy.n.01'), Synset('economy.n.02'), Synset('economy.n.03')]\n",
      "\n",
      "****************************************************************************************************\n",
      "soldiers\n",
      "Word2Vec Results\n",
      "[('traders', 0.7417430281639099), ('shores', 0.6989766955375671), ('indignant', 0.69255131483078), ('French', 0.6835082173347473), ('girls', 0.6714829206466675)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('soldier.n.01'), Synset('soldier.n.02'), Synset('soldier.v.01')]\n",
      "\n",
      "****************************************************************************************************\n",
      "dogs\n",
      "Word2Vec Results\n",
      "[('brightly', 0.6037564277648926), ('rain', 0.5853180885314941), ('Kearton', 0.5801936388015747), ('sidewalks', 0.5601804256439209), ('horses', 0.5507915019989014)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03')]\n",
      "\n",
      "****************************************************************************************************\n",
      "hospital\n",
      "Word2Vec Results\n",
      "[('pennant', 0.8942158222198486), ('council', 0.8880501389503479), ('Legislature', 0.8816936016082764), ('league', 0.8768377304077148), ('Rules', 0.8674756288528442)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('hospital.n.01'), Synset('hospital.n.02')]\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "nouns = [\"students\", \"economy\", \"soldiers\", \"dogs\", \"hospital\"]\n",
    "for n in nouns:\n",
    "    print(n)\n",
    "    \n",
    "    print(\"Word2Vec Results\")\n",
    "    print(brown_model.most_similar(n)[:5])\n",
    "    print()\n",
    "    \n",
    "    print(\"WordNet Results\")\n",
    "    print(wn.synsets(n.lower())[:3])\n",
    "    print()\n",
    "    \n",
    "    print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet seems to provide a better result for each of the words because Word2Vec seems to have high recall but also low precision. For example, for \"students\", Word2Vec returned one similar word: \"enroll\". However, the remaining words: automobiles', 'matters','countries', and 'counterparts' are not very similar to student. This same pattern was apparent with the remaining of the nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Do the same for 5 adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greatly\n",
      "Word2Vec Results\n",
      "[('radar', 0.7545557022094727), ('guided', 0.7503126859664917), ('donated', 0.7400814890861511), ('exploited', 0.7389732599258423), ('investigated', 0.7284181118011475)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('greatly.r.01')]\n",
      "\n",
      "****************************************************************************************************\n",
      "economy\n",
      "Word2Vec Results\n",
      "[('outlook', 0.8700981736183167), ('union', 0.8654215335845947), ('acceptance', 0.8580105304718018), ('leadership', 0.853107213973999), ('commitment', 0.8510946035385132)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('economy.n.01'), Synset('economy.n.02'), Synset('economy.n.03')]\n",
      "\n",
      "****************************************************************************************************\n",
      "soldiers\n",
      "Word2Vec Results\n",
      "[('traders', 0.7417430281639099), ('shores', 0.6989766955375671), ('indignant', 0.69255131483078), ('French', 0.6835082173347473), ('girls', 0.6714829206466675)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('soldier.n.01'), Synset('soldier.n.02'), Synset('soldier.v.01')]\n",
      "\n",
      "****************************************************************************************************\n",
      "dogs\n",
      "Word2Vec Results\n",
      "[('brightly', 0.6037564277648926), ('rain', 0.5853180885314941), ('Kearton', 0.5801936388015747), ('sidewalks', 0.5601804256439209), ('horses', 0.5507915019989014)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03')]\n",
      "\n",
      "****************************************************************************************************\n",
      "hospital\n",
      "Word2Vec Results\n",
      "[('pennant', 0.8942158222198486), ('council', 0.8880501389503479), ('Legislature', 0.8816936016082764), ('league', 0.8768377304077148), ('Rules', 0.8674756288528442)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('hospital.n.01'), Synset('hospital.n.02')]\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "adjectives = [\"greatly\", \"economy\", \"soldiers\", \"dogs\", \"hospital\"]\n",
    "\n",
    "for a in adjectives:\n",
    "    print(a)\n",
    "    \n",
    "    print(\"Word2Vec Results\")\n",
    "    print(brown_model.most_similar(a)[:5])\n",
    "    print()\n",
    "    \n",
    "    print(\"WordNet Results\")\n",
    "    print(wn.synsets(a.lower())[:3])\n",
    "    print()\n",
    "    \n",
    "    print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from \"economy\", Word2Vec also seemed to struggle with adjectives as it did with the nouns. It seems to be tightly coupled with the dataset that is passed to it. In this case, it is closely tied to the brown dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Do the same for 5 verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climb\n",
      "Word2Vec Results\n",
      "[('grow', 0.7233080863952637), ('add', 0.7189280986785889), ('decide', 0.7062013149261475), ('drain', 0.6854236125946045), ('throw', 0.6576958894729614)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('ascent.n.01'), Synset('climb.n.02'), Synset('climb.n.03')]\n",
      "\n",
      "****************************************************************************************************\n",
      "assume\n",
      "Word2Vec Results\n",
      "[('suggest', 0.8785237669944763), ('recognize', 0.8768250346183777), ('consider', 0.8590641021728516), ('realize', 0.8582191467285156), ('deny', 0.8558826446533203)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('assume.v.01'), Synset('assume.v.02'), Synset('assume.v.03')]\n",
      "\n",
      "****************************************************************************************************\n",
      "translated\n",
      "Word2Vec Results\n",
      "[('preventing', 0.7863573431968689), ('patches', 0.7138186693191528), ('extremely', 0.7026211023330688), ('outward', 0.6979726552963257), ('plentiful', 0.6964269876480103)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('translate.v.01'), Synset('translate.v.02'), Synset('understand.v.03')]\n",
      "\n",
      "****************************************************************************************************\n",
      "create\n",
      "Word2Vec Results\n",
      "[('achieve', 0.9144303798675537), ('satisfy', 0.8891776204109192), ('illustrate', 0.868241548538208), ('justify', 0.8597403168678284), ('ascertain', 0.859681248664856)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('make.v.03'), Synset('create.v.02'), Synset('create.v.03')]\n",
      "\n",
      "****************************************************************************************************\n",
      "interview\n",
      "Word2Vec Results\n",
      "[('statue', 0.8156107664108276), ('firmer', 0.8065776824951172), ('myriad', 0.8060436844825745), ('shattered', 0.7881370186805725), ('pastor', 0.7812305688858032)]\n",
      "\n",
      "WordNet Results\n",
      "[Synset('interview.n.01'), Synset('consultation.n.01'), Synset('interview.v.01')]\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "verbs = [\"climb\", \"assume\", \"translated\", \"create\", \"interview\"]\n",
    "for v in verbs:\n",
    "    print(v)\n",
    "    \n",
    "    print(\"Word2Vec Results\")\n",
    "    print(brown_model.most_similar(v)[:5])\n",
    "    print()\n",
    "    \n",
    "    print(\"WordNet Results\")\n",
    "    print(wn.synsets(v.lower())[:3])\n",
    "    print()\n",
    "    \n",
    "    print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with verbs, Wordnet still seems to be superior because Word2Vec still returns too may words that are dissimmilar despite having high similarity scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
